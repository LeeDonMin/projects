{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e818cd7d",
   "metadata": {},
   "source": [
    "# 술에 취한 딱정벌레\n",
    "\n",
    "딱정벌레가 N X N 크기의 방을 움직이고 있다.\n",
    "\n",
    "가운데 점을 시작 위치로, 딱정벌레가 술에 취해 상하좌우 4방향을 무작위로 움직인다.\n",
    "\n",
    "딱정벌레가 방안의 모든 타일을 한 번 이상 지나가는데 걸리는 총 이동수는 얼마인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8b2ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=2\n",
    "N=2*x+1\n",
    "room = np.zeros(shape=(N,N),dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af8d5c",
   "metadata": {},
   "source": [
    "### 심화) 딱장벌레가 모든 타일을 지나갔을 때, 각각의 타일을 몇 번 지나갔는지 숫자로 표시하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed58306",
   "metadata": {},
   "outputs": [],
   "source": [
    "room[x,x]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46febfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = x\n",
    "j = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93d8f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "movelist = ['w','s','a','d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "x=2\n",
    "N=2*x+1\n",
    "room = np.zeros(shape=(N,N),dtype=np.int64)\n",
    "movelist = ['w','s','a','d']\n",
    "i = x\n",
    "j = x\n",
    "\n",
    "def randommove(ㅋ, ㅌ):\n",
    "    move = random.choice(movelist)\n",
    "    p = ㅋ\n",
    "    q = ㅌ\n",
    "    if move =='w':\n",
    "        ㅌ-=1\n",
    "    elif move == 's':\n",
    "        ㅌ+=1\n",
    "    elif move == 'a':\n",
    "        ㅋ-=1\n",
    "    else:\n",
    "        ㅋ+=1\n",
    "\n",
    "    if (ㅌ>=N) | (ㅋ>=N) |(ㅌ<0)|(ㅋ<0):\n",
    "        ㅋ,ㅌ = randommove(p,q)\n",
    "        return ㅋ,ㅌ\n",
    "    else:\n",
    "        return ㅋ, ㅌ\n",
    "m=0\n",
    "n=0\n",
    "while True:\n",
    "    room = np.zeros(shape=(N,N),dtype=np.int64)\n",
    "    while (np.where(room==0)[0].shape!=(0,)):\n",
    "        # clear_output()\n",
    "        m+=1\n",
    "        room[j,i]+=1\n",
    "        # print(room)\n",
    "        # print(i, j)\n",
    "        # time.sleep(1/144)\n",
    "        i, j=randommove(i,j)\n",
    "    if m >50:\n",
    "        m=0\n",
    "    elif m <=50:\n",
    "        break\n",
    "    n+=1\n",
    "print(room)\n",
    "print(i, j)\n",
    "print(m,'회')\n",
    "print(n,'루프')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c542e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b533a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(room==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "d=4\n",
    "t,d=randommove(t,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(randommove(0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12e4e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "room=room+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "room[j+1,i+1]=2\n",
    "np.where(room==0)[0].shape==(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "DATASET_PATH = 'Google_Recaptcha_V2_Images_Dataset/images/'  # 예: 'dataset/'\n",
    "\n",
    "# 클래스 이름 추출 (폴더 이름)\n",
    "class_names = sorted(os.listdir(DATASET_PATH))\n",
    "num_classes = len(class_names)\n",
    "print(f\"클래스 수: {num_classes}\")\n",
    "print(f\"클래스 이름: {class_names}\")\n",
    "\n",
    "# 이미지와 레이블 저장할 리스트 초기화\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "# 데이터 로드 함수\n",
    "def load_dataset(dataset_path, class_names, img_height, img_width):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(dataset_path, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            # 이미지 읽기\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue  # 이미지 로드 실패 시 건너뛰기\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGR에서 RGB로 변환\n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# 데이터 로드\n",
    "images, labels = load_dataset(DATASET_PATH, class_names, IMG_HEIGHT, IMG_WIDTH)\n",
    "print(f\"총 이미지 수: {len(images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f05159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "# 이미지 파일 경로 (이미지 파일들이 들어있는 폴더 경로)\n",
    "DATASET_PATH = 'Google_Recaptcha_V2_Images_Dataset/images/'  # 예: 'dataset/'\n",
    "class_names = sorted(os.listdir(DATASET_PATH))\n",
    "num_classes = len(class_names)\n",
    "print(f\"클래스 수: {num_classes}\")\n",
    "print(f\"클래스 이름: {class_names}\")\n",
    "\n",
    "# 클래스 목록 (사용자가 정의할 수 있음)\n",
    "classes = class_names\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# 확대 배율 (라벨링할 때 이미지를 크게 보이도록 설정)\n",
    "SCALE_FACTOR = 4  # 예: 4배 확대\n",
    "\n",
    "# 이미지를 확대하여 표시하고, bounding box 좌표를 원본 이미지에 맞게 조정하는 함수\n",
    "def label_image(image):\n",
    "    \"\"\"\n",
    "    OpenCV 창에서 사용자가 bounding box를 지정하고, 라벨을 선택한 후 데이터를 저장.\n",
    "    이미지는 확대하여 표시되고, bounding box는 원본 크기에 맞게 조정됨.\n",
    "    \"\"\"\n",
    "    # 원본 이미지 크기\n",
    "    orig_height, orig_width = image.shape[:2]\n",
    "    \n",
    "    # 이미지를 확대하여 표시 (배율 만큼 확대)\n",
    "    large_image = cv2.resize(image, (orig_width * SCALE_FACTOR, orig_height * SCALE_FACTOR))\n",
    "    \n",
    "    # OpenCV로 bounding box 지정 (사용자가 마우스로 선택)\n",
    "    bboxes = []\n",
    "    \n",
    "    while True:\n",
    "        bbox = cv2.selectROI(\"bounding box (ESC key = end)\", large_image, fromCenter=False, showCrosshair=True)\n",
    "        if bbox[2] == 0 or bbox[3] == 0:  # ESC 키를 눌러 선택을 종료한 경우\n",
    "            break\n",
    "        bboxes.append(bbox)\n",
    "\n",
    "        # 선택한 영역을 시각적으로 보여주기\n",
    "        plt.imshow(large_image)\n",
    "        rect = plt.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.title(f\"check your select\")\n",
    "        print(f\"클래스 이름: {class_names}\")\n",
    "        plt.show()\n",
    "    \n",
    "    # bounding box 지정 후 OpenCV 창 닫기\n",
    "    cv2.destroyAllWindows()  # OpenCV 창을 닫은 후 콘솔 입력을 받을 수 있도록 처리\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    # 각 bounding box에 대해 클래스 선택\n",
    "    for bbox in bboxes:\n",
    "        # 사용자에게 클래스 선택 받기\n",
    "        print(\"클래스를 선택하세요: \")\n",
    "        for i, cls in enumerate(classes):\n",
    "            print(f\"{i}: {cls}\")\n",
    "        class_idx = input(\"클래스 인덱스를 입력하세요 (중단하려면 'exit' 입력): \")\n",
    "        \n",
    "        if class_idx == 'exit':\n",
    "            print(\"라벨링 작업을 중단합니다.\")\n",
    "            return None  # 작업 중단\n",
    "        \n",
    "        class_idx = int(class_idx)\n",
    "        \n",
    "        # Bounding box 좌표를 원본 크기로 변환\n",
    "        bbox_resized = (\n",
    "            bbox[0] / SCALE_FACTOR,  # x 좌표\n",
    "            bbox[1] / SCALE_FACTOR,  # y 좌표\n",
    "            bbox[2] / SCALE_FACTOR,  # 폭 (width)\n",
    "            bbox[3] / SCALE_FACTOR   # 높이 (height)\n",
    "        )\n",
    "        \n",
    "        # Bounding box 좌표와 클래스 라벨 저장\n",
    "        bbox_normalized = (\n",
    "            bbox_resized[0] / orig_width, \n",
    "            bbox_resized[1] / orig_height, \n",
    "            bbox_resized[2] / orig_width, \n",
    "            bbox_resized[3] / orig_height\n",
    "        )  # 이미지 크기로 정규화된 좌표\n",
    "        label = [class_idx] + list(bbox_normalized)  # [클래스, x, y, w, h]\n",
    "        labels.append(label)\n",
    "\n",
    "        # 입력이 끝난 후 라벨링 결과 확인\n",
    "        plt.imshow(image)\n",
    "        rect = plt.Rectangle((bbox_normalized[0] * orig_width, bbox_normalized[1] * orig_height), \n",
    "                             bbox_normalized[2] * orig_width, bbox_normalized[3] * orig_height, \n",
    "                             edgecolor='b', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.title(f\"class: {classes[class_idx]} - check\")\n",
    "        plt.show()\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# 이미지 폴더 내의 모든 이미지에 대해 라벨링 작업 수행\n",
    "for label, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(DATASET_PATH, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        clear_output()\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        # 이미지 읽기\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue  # 이미지 로드 실패 시 건너뛰기\n",
    "        labels = label_image(img)  # 여러 bounding box 라벨링\n",
    "        check_next = input(\"계속 하시겠습니까? yes/ no:\")\n",
    "        if check_next =='yes':\n",
    "            continue\n",
    "        elif check_next == 'no':\n",
    "            X_train.append(img)\n",
    "            y_train.extend(labels)\n",
    "            break\n",
    "\n",
    "        if labels is None:  # 중단된 경우\n",
    "            break\n",
    "        X_train.append(img)\n",
    "        y_train.append(labels)  # 여러 bounding box가 있을 수 있으므로 extend 사용\n",
    "        # cv2.destroyAllWindows()를 여기서 호출하지 않음, 이미 label_image 함수에서 처리됨\n",
    "    if check_next =='yes':\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 데이터를 NumPy 배열로 변환 (모델 학습을 위해)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# 데이터 출력\n",
    "print(\"X_train shape:\", X_train.shape)  # 이미지 데이터 (예: (N, 224, 224, 3))\n",
    "print(\"y_train shape:\", y_train.shape)  # 라벨 데이터 (클래스 + bounding box)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf34bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 이미지 파일 경로 (이미지 파일들이 들어있는 폴더 경로)\n",
    "DATASET_PATH = 'Google_Recaptcha_V2_Images_Dataset/images/'\n",
    "class_names = sorted(os.listdir(DATASET_PATH))\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'orange', 'purple', 'pink', 'brown', 'gray']  # 'tab20' 컬러맵에서 12개의 색상을 추출\n",
    "\n",
    "# 클래스 목록 (사용자가 정의할 수 있음)\n",
    "classes = class_names\n",
    "X_train = []\n",
    "y_train = []  # 이미지별로 라벨을 딕셔너리 형태로 저장할 리스트\n",
    "\n",
    "# 확대 배율 (라벨링할 때 이미지를 크게 보이도록 설정)\n",
    "SCALE_FACTOR = 4  # 예: 4배 확대\n",
    "\n",
    "# 이미지를 확대하여 표시하고, bounding box 좌표를 원본 이미지에 맞게 조정하는 함수\n",
    "def label_image(image):\n",
    "    orig_height, orig_width = image.shape[:2]\n",
    "    large_image = cv2.resize(image, (orig_width * SCALE_FACTOR, orig_height * SCALE_FACTOR))\n",
    "\n",
    "    # OpenCV로 bounding box 지정 (사용자가 마우스로 선택)\n",
    "    bboxes = []\n",
    "    while True:\n",
    "        bbox = cv2.selectROI(f\"bounding box{img_name} (ESC key = end)\", large_image, fromCenter=False, showCrosshair=True)\n",
    "        if bbox[2] == 0 or bbox[3] == 0:  # ESC 키를 눌러 선택을 종료한 경우\n",
    "            break\n",
    "        bboxes.append(bbox)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    labels = []\n",
    "    for bbox in bboxes:\n",
    "        for i, cls in enumerate(classes):\n",
    "            print(f\"{i}: {cls}\")\n",
    "        plt.imshow(large_image)\n",
    "        rect = plt.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.title(f\"check your select{img_name}\")\n",
    "\n",
    "        plt.show()\n",
    "        print(f\"클래스 이름: {class_names}\")\n",
    "        print(\"클래스를 선택하세요: \")\n",
    "        class_idx = input(\"클래스 인덱스를 입력하세요 (중단하려면 'exit' 입력): \")\n",
    "        clear_output() \n",
    "        if class_idx == 'exit':\n",
    "            print(\"라벨링 작업을 중단합니다.\")\n",
    "            return None  # 작업 중단\n",
    "\n",
    "        class_idx = int(class_idx)\n",
    "        \n",
    "        # Bounding box 좌표를 원본 크기로 변환\n",
    "        bbox_resized = (\n",
    "            bbox[0] / SCALE_FACTOR,  # x 좌표\n",
    "            bbox[1] / SCALE_FACTOR,  # y 좌표\n",
    "            bbox[2] / SCALE_FACTOR,  # 폭 (width)\n",
    "            bbox[3] / SCALE_FACTOR   # 높이 (height)\n",
    "        )\n",
    "        \n",
    "        # Bounding box 좌표를 원본 이미지 크기로 정규화\n",
    "        bbox_normalized = {\n",
    "            \"class\": class_idx,\n",
    "            \"bbox\": [\n",
    "                bbox_resized[0],  # x 좌표 (정규화)\n",
    "                bbox_resized[1],  # y 좌표 (정규화)\n",
    "                bbox_resized[2],  # 폭 (정규화)\n",
    "                bbox_resized[3]  # 높이 (정규화)\n",
    "            ]\n",
    "        }\n",
    "        labels.append(bbox_normalized)\n",
    "        plt.imshow(image)\n",
    "        color = colors[class_idx % len(colors)]\n",
    "        rect = plt.Rectangle((bbox_normalized['bbox'][0], bbox_normalized['bbox'][1]), \n",
    "                             bbox_normalized['bbox'][2], bbox_normalized['bbox'][3], \n",
    "                             edgecolor=color, facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.title(f\"class: {classes[class_idx]} - check\")\n",
    "        plt.show()\n",
    "    return labels\n",
    "\n",
    "# 이미지 폴더 내의 모든 이미지에 대해 라벨링 작업 수행\n",
    "for label, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(DATASET_PATH, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        # 이미지 읽기\n",
    "        clear_output() \n",
    "        img = cv2.imread(img_path)\n",
    "        print(class_names)\n",
    "        if img is None:\n",
    "            continue  # 이미지 로드 실패 시 건너뛰기\n",
    "        labels = label_image(img)  # 여러 bounding box 라벨링\n",
    "        if labels == None:\n",
    "            check_user = input('더 할꺼임? no치면 작업 완전 종료 other 치면 다른 디렉토리로 이동')\n",
    "            if check_user == 'no':\n",
    "                check_next ='no'\n",
    "                break\n",
    "            elif check_user == 'other':\n",
    "                check_next = 'other'\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        check_next = input(\"계속 하시겠습니까? yes/ no/ other:\")\n",
    "        if check_next == 'no' or check_next=='other':\n",
    "            X_train.append(img)\n",
    "            y_train.append({\"image_id\": img_name, \"objects\": labels}) \n",
    "            break\n",
    "        else:\n",
    "            X_train.append(img)\n",
    "            y_train.append({\"image_id\": img_name, \"objects\": labels}) \n",
    "            \n",
    "            continue\n",
    "\n",
    "    if check_next == 'no':\n",
    "        break\n",
    "    elif check_next =='other':\n",
    "        continue\n",
    "\n",
    "\n",
    "# 예시: y_train의 구조 확인\n",
    "for data in y_train:  # 첫 두 이미지의 라벨 확인\n",
    "    print(f\"이미지 파일: {data['image_id']}\")\n",
    "    for obj in data[\"objects\"]:\n",
    "        print(f\"  클래스: {obj['class']}, bounding box: {obj['bbox']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87ce43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "# YOLO 모델 입력 크기\n",
    "IMG_SIZE = 120  # 입력 이미지 크기 (120x120으로 맞춤)\n",
    "GRID_SIZE = 7   # 7x7 그리드로 나누어 예측\n",
    "NUM_CLASSES = 12  # 클래스 수 (12개의 클래스)\n",
    "BOXES_PER_CELL = 2  # 각 그리드 셀당 예측하는 박스의 수\n",
    "\n",
    "def preprocess_data(X_train, y_train):\n",
    "    \"\"\"\n",
    "    X_train: 이미지 배열 (리스트 형태)\n",
    "    y_train: 각 이미지에 대한 라벨 정보 (클래스 및 bounding box)\n",
    "\n",
    "    이미지와 라벨 데이터를 YOLO 스타일로 전처리\n",
    "    \"\"\"\n",
    "    num_images = len(X_train)\n",
    "    X_processed = np.zeros((num_images, IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "    y_processed = np.zeros((num_images, GRID_SIZE, GRID_SIZE, BOXES_PER_CELL, 5 + NUM_CLASSES), dtype=np.float32)\n",
    "    \n",
    "    for i, (image, label_data) in enumerate(zip(X_train, y_train)):\n",
    "        # 이미지 크기 맞추기 (120x120)\n",
    "        resized_image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE)).numpy()\n",
    "        X_processed[i] = resized_image / 255.0  # 이미지 정규화 (0~1)\n",
    "\n",
    "        for obj in label_data['objects']:\n",
    "            class_idx = obj['class']\n",
    "            bbox = obj['bbox']\n",
    "\n",
    "            # bounding box 좌표 변환 (이미지 크기를 기준으로 절대 좌표를 YOLO 그리드 기준으로 변환)\n",
    "            x_center = (bbox[0] + bbox[2] / 2) * GRID_SIZE  # x 중심 좌표 (정규화된 좌표 * GRID_SIZE)\n",
    "            y_center = (bbox[1] + bbox[3] / 2) * GRID_SIZE  # y 중심 좌표 (정규화된 좌표 * GRID_SIZE)\n",
    "\n",
    "            # 그리드 인덱스를 구할 때, 값이 범위를 초과하지 않도록 제한\n",
    "            grid_x = min(int(x_center), GRID_SIZE - 1)\n",
    "            grid_y = min(int(y_center), GRID_SIZE - 1)\n",
    "\n",
    "            box_x = x_center - grid_x\n",
    "            box_y = y_center - grid_y\n",
    "\n",
    "            # bounding box 크기 (width, height)\n",
    "            box_w = bbox[2]\n",
    "            box_h = bbox[3]\n",
    "\n",
    "            # 라벨 저장 (클래스 정보와 bounding box)\n",
    "            y_processed[i, grid_y, grid_x, 0, 0:4] = [box_x, box_y, box_w, box_h]  # bounding box 좌표\n",
    "            y_processed[i, grid_y, grid_x, 0, 4] = 1  # 객체가 존재함\n",
    "            y_processed[i, grid_y, grid_x, 0, 5 + class_idx] = 1  # 클래스 정보\n",
    "\n",
    "    return X_processed, y_processed\n",
    "\n",
    "\n",
    "# 전처리된 데이터\n",
    "X_processed, y_processed = preprocess_data(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c73c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yolo_model():\n",
    "    \"\"\"\n",
    "    YOLO 스타일 Object Detection 모델 정의\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    # 간단한 CNN 블록\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # YOLO 마지막 층: GRID_SIZE x GRID_SIZE x (5 + NUM_CLASSES) 출력\n",
    "    output_shape = (GRID_SIZE, GRID_SIZE, BOXES_PER_CELL, 5 + NUM_CLASSES)  # 5 = (x, y, w, h, confidence)\n",
    "    outputs = layers.Conv2D(BOXES_PER_CELL * (5 + NUM_CLASSES), (1, 1), activation='linear')(x)\n",
    "    outputs = layers.Reshape(output_shape)(outputs)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# YOLO 모델 생성\n",
    "model = create_yolo_model()\n",
    "\n",
    "# 모델 요약\n",
    "model.summary()\n",
    "\n",
    "# YOLO 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f80c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "model.fit(X_processed, y_processed, epochs=100, batch_size=8)\n",
    "\n",
    "# 학습이 끝나면 모델 저장\n",
    "model.save(\"yolo_object_detection_model_120x120.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([X_train[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240fdd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(image, y_pred, threshold=0.1):\n",
    "    \"\"\"\n",
    "    모델의 예측 결과를 시각화하는 함수.\n",
    "    \n",
    "    image: 예측할 이미지 (shape: (120, 120, 3))\n",
    "    y_pred: YOLO 모델의 예측 결과 (shape: (7, 7, 2, 5 + NUM_CLASSES))\n",
    "    threshold: confidence score가 이 값 이상인 박스만 표시\n",
    "    \"\"\"\n",
    "    GRID_SIZE = y_pred.shape[0]\n",
    "    \n",
    "    # 이미지 시각화\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # 그리드 셀에서 객체 탐지된 bounding box 그리기\n",
    "    for grid_y in range(GRID_SIZE):\n",
    "        for grid_x in range(GRID_SIZE):\n",
    "            for b in range(BOXES_PER_CELL):\n",
    "                box = y_pred[grid_y, grid_x, b]\n",
    "                \n",
    "                confidence = box[4]  # confidence score\n",
    "                if confidence < threshold:\n",
    "                    continue  # threshold 이하인 박스는 무시\n",
    "                \n",
    "                # bounding box 좌표 복원\n",
    "                box_x, box_y, box_w, box_h = box[0:4]\n",
    "                box_x = (grid_x + box_x) * (image.shape[1] / GRID_SIZE)  # 이미지 크기에 맞춰 복원\n",
    "                box_y = (grid_y + box_y) * (image.shape[0] / GRID_SIZE)\n",
    "                box_w *= image.shape[1]\n",
    "                box_h *= image.shape[0]\n",
    "                \n",
    "                # 가장 높은 클래스 확률 찾기\n",
    "                class_probs = box[5:]\n",
    "                class_idx = np.argmax(class_probs)  # 가장 높은 확률을 가진 클래스 선택\n",
    "                class_name = classes[class_idx]  # 클래스 이름 가져오기\n",
    "                \n",
    "                # bounding box 그리기\n",
    "                rect = plt.Rectangle((box_x - box_w / 2, box_y - box_h / 2), box_w, box_h, \n",
    "                                     edgecolor='r', facecolor='none', linewidth=2)\n",
    "                plt.gca().add_patch(rect)\n",
    "                \n",
    "                # plt.text(box_x - box_w / 2, box_y - box_h / 2 - 10, f\"{class_name}: {confidence:.2f}\", \n",
    "                #          color='r', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n",
    "    \n",
    "    # 시각화 결과 표시\n",
    "    print(f\"{class_name}: {confidence:.2f}\")\n",
    "    plt.show()\n",
    "\n",
    "# 예측할 이미지 (배치에서 하나의 이미지를 가져옴)\n",
    "test_image = X_processed[0]  # 0번째 이미지를 예측하는 예시\n",
    "\n",
    "# 모델로 예측 수행\n",
    "y_pred = model.predict(test_image[np.newaxis, ...])[0]  # 모델은 배치 단위로 예측하므로 배치 차원 추가\n",
    "\n",
    "# 예측 결과 시각화\n",
    "visualize_predictions(test_image, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16772e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwerqwerq=model.predict(np.array([X_train[0]]))\n",
    "qwerqwerq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790967bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in y_train:  # 첫 두 이미지의 라벨 확인\n",
    "    print(f\"이미지 파일: {data['image_id']}\")\n",
    "    for obj in data[\"objects\"]:\n",
    "        print(f\"  클래스: {obj['class']}, bounding box: {obj['bbox']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "# 강화학습 환경 정의 (HumanObjectDetectionEnv)\n",
    "class HumanObjectDetectionEnv:\n",
    "    def __init__(self, image, ground_truth_bboxes):\n",
    "        self.image = image\n",
    "        self.ground_truth_bboxes = ground_truth_bboxes  # 실제 bounding box들 (유저가 선택한 여러 개)\n",
    "        self.current_bbox = self.random_bbox()  # 초기 랜덤 bounding box\n",
    "        self.done = False\n",
    "\n",
    "    def random_bbox(self):\n",
    "        # 랜덤 bounding box 생성\n",
    "        return np.random.uniform(0.1, 0.9, 4)\n",
    "\n",
    "    def calculate_iou(self, box1, box2):\n",
    "        # IoU 계산\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n",
    "        y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n",
    "        \n",
    "        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        box1_area = box1[2] * box1[3]\n",
    "        box2_area = box2[2] * box2[3]\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        \n",
    "        return inter_area / union_area  # IoU 반환\n",
    "\n",
    "    def step(self, action):\n",
    "        # 강화학습 에이전트의 action에 따른 bounding box 이동\n",
    "        if action == 0:   # 왼쪽 이동\n",
    "            self.current_bbox[0] = max(0, self.current_bbox[0] - 0.05)\n",
    "        elif action == 1: # 오른쪽 이동\n",
    "            self.current_bbox[0] = min(1 - self.current_bbox[2], self.current_bbox[0] + 0.05)\n",
    "        elif action == 2: # 위로 이동\n",
    "            self.current_bbox[1] = max(0, self.current_bbox[1] - 0.05)\n",
    "        elif action == 3: # 아래로 이동\n",
    "            self.current_bbox[1] = min(1 - self.current_bbox[3], self.current_bbox[1] + 0.05)\n",
    "\n",
    "        # bounding box 시각화 (환경 렌더링)\n",
    "        self.render()\n",
    "\n",
    "        # 각 ground truth 박스들과 IoU 계산, 최대값 사용\n",
    "        max_iou = 0\n",
    "        for gt_bbox in self.ground_truth_bboxes:\n",
    "            iou = self.calculate_iou(self.current_bbox, gt_bbox['bbox'])\n",
    "            max_iou = max(max_iou, iou)\n",
    "\n",
    "        # 목표 도달 여부 판단\n",
    "        if max_iou > 0.9:  # IoU 0.9 이상이면 목표 도달\n",
    "            self.done = True\n",
    "            print(f\"목표 도달! IoU: {max_iou}\")\n",
    "        else:\n",
    "            print(f\"현재 IoU: {max_iou}\")\n",
    "        \n",
    "        return self.current_bbox, max_iou, self.done\n",
    "\n",
    "    def render(self):\n",
    "        # Matplotlib으로 bounding box 시각화\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(self.image)\n",
    "\n",
    "        # 현재 bounding box 그리기\n",
    "        current_rect = Rectangle((self.current_bbox[0] * self.image.shape[1], \n",
    "                                  self.current_bbox[1] * self.image.shape[0]), \n",
    "                                 self.current_bbox[2] * self.image.shape[1], \n",
    "                                 self.current_bbox[3] * self.image.shape[0], \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(current_rect)\n",
    "\n",
    "        # 유저가 선택한 ground truth bounding boxes 그리기\n",
    "        for gt_bbox in self.ground_truth_bboxes:\n",
    "            gt_rect = Rectangle((gt_bbox['bbox'][0] * self.image.shape[1], \n",
    "                                 gt_bbox['bbox'][1] * self.image.shape[0]), \n",
    "                                gt_bbox['bbox'][2] * self.image.shape[1], \n",
    "                                gt_bbox['bbox'][3] * self.image.shape[0], \n",
    "                                linewidth=2, edgecolor='g', facecolor='none')\n",
    "            ax.add_patch(gt_rect)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# OpenCV로 사용자에게 여러 bounding box를 선택하게 하는 함수\n",
    "def get_user_defined_bboxes(image):\n",
    "    \"\"\"\n",
    "    여러 개의 bounding box를 선택하고, 각각의 클래스 지정.\n",
    "    \"\"\"\n",
    "    bboxes = []\n",
    "    while True:\n",
    "        # OpenCV 창에서 사용자가 bounding box 선택\n",
    "        bbox = cv2.selectROI(\"이미지에서 bounding box를 선택하세요 (ESC 키로 종료)\", image, fromCenter=False, showCrosshair=True)\n",
    "        if bbox[2] == 0 or bbox[3] == 0:  # ESC 키를 눌러 선택을 종료한 경우\n",
    "            break\n",
    "        # (x, y, w, h) -> 정규화된 bounding box (비율)\n",
    "        bbox_normalized = {\n",
    "            \"bbox\": [\n",
    "                bbox[0] / image.shape[1], bbox[1] / image.shape[0], \n",
    "                bbox[2] / image.shape[1], bbox[3] / image.shape[0]\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # 각 bounding box에 대해 클래스 입력 받기\n",
    "        print(f\"클래스를 선택하세요: \")\n",
    "        for i, cls in enumerate(classes):\n",
    "            print(f\"{i}: {cls}\")\n",
    "        class_idx = int(input(\"클래스 인덱스를 입력하세요: \"))\n",
    "        bbox_normalized[\"class\"] = class_idx\n",
    "        \n",
    "        bboxes.append(bbox_normalized)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "# 강화학습 에이전트 정의 (DDQN 등)\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # 할인율\n",
    "        self.epsilon = 1.0   # 탐험 비율 (exploration)\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # 간단한 Neural Network 모델 구성\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Target Model 업데이트 (DDQN 특징)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        # 탐험 또는 최적 행동 선택\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # Q값이 최대인 행동 선택\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # 경험 저장\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # 메모리에서 샘플 추출하여 학습\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# 실행\n",
    "DATASET_PATH = 'Google_Recaptcha_V2_Images_Dataset/images/'\n",
    "class_names = sorted(os.listdir(DATASET_PATH))\n",
    "for label, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(DATASET_PATH, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        # 이미지 읽기\n",
    "        clear_output() \n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "# 유저가 직접 bounding box를 선택 (여러 개)\n",
    "        user_defined_bboxes = get_user_defined_bboxes(image)\n",
    "        print(f\"유저가 선택한 bounding boxes: {user_defined_bboxes}\")\n",
    "\n",
    "        # 강화학습 환경에 유저가 정의한 bounding box 사용\n",
    "        env = HumanObjectDetectionEnv(image, user_defined_bboxes)\n",
    "\n",
    "        # 강화학습 에이전트 생성 및 학습 과정\n",
    "        state_size = 4  # bounding box의 (x, y, width, height)\n",
    "        action_size = 4  # 4가지 행동 (위, 아래, 왼쪽, 오른쪽)\n",
    "        agent = DDQNAgent(state_size, action_size)\n",
    "\n",
    "        episodes = 10\n",
    "        for e in range(episodes):\n",
    "            state = env.random_bbox()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            \n",
    "            for time in range(200):\n",
    "                # 에이전트가 행동 선택\n",
    "                action = agent.act(state)\n",
    "                \n",
    "                # 환경에 action 적용하고 다음 상태, 보상, 종료 여부 받기\n",
    "                next_state, reward, done = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                \n",
    "                # 에이전트에게 경험 저장\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                # 상태 업데이트\n",
    "                state = next_state\n",
    "                clear_output(wait=True)\n",
    "                if done:\n",
    "                    agent.update_target_model()\n",
    "                    print(f\"Episode: {e}/{episodes}, Score: {reward}\")\n",
    "                    break\n",
    "                \n",
    "            if len(agent.memory) > 32:\n",
    "                agent.replay(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf9a322",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:536\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     fh \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mfileno()\n\u001b[0;32m    537\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 522\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDDQN 에이전트의 모델이 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAGENT_SAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m에 저장되었습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 522\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 476\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# RL 에이전트 학습\u001b[39;00m\n\u001b[0;32m    475\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m--> 476\u001b[0m run_combined_model(img, conv_model, agent, env, target_class, episodes\u001b[38;5;241m=\u001b[39mepisodes, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# 시각화: 사용자, Conv2D, RL 에이전트의 바운딩 박스 표시\u001b[39;00m\n\u001b[0;32m    479\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n",
      "Cell \u001b[1;32mIn[1], line 380\u001b[0m, in \u001b[0;36mrun_combined_model\u001b[1;34m(image, conv2d_model, agent, env, class_idx, episodes, batch_size)\u001b[0m\n\u001b[0;32m    377\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# 환경에 action 적용하고 다음 상태, 보상, 종료 여부 받기\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    381\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(next_state, [\u001b[38;5;241m1\u001b[39m, agent\u001b[38;5;241m.\u001b[39mstate_size])\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# 에이전트에게 경험 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 338\u001b[0m, in \u001b[0;36mHumanObjectDetectionEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    335\u001b[0m     reward \u001b[38;5;241m=\u001b[39m max_iou  \u001b[38;5;66;03m# IoU를 보상으로 사용\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# bounding box 시각화 (환경 렌더링)\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_bbox, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone\n",
      "Cell \u001b[1;32mIn[1], line 368\u001b[0m, in \u001b[0;36mHumanObjectDetectionEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    366\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    367\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m현재 정확도:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miou\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 368\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:527\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    526\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         display(\n\u001b[0;32m     91\u001b[0m             figure_manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[0;32m     92\u001b[0m             metadata\u001b[38;5;241m=\u001b[39m_fetch_figure_metadata(figure_manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure)\n\u001b[0;32m     93\u001b[0m         )\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m(obj, include\u001b[38;5;241m=\u001b[39minclude, exclude\u001b[38;5;241m=\u001b[39mexclude)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:182\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    180\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     data \u001b[38;5;241m=\u001b[39m formatter(obj)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:226\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 226\u001b[0m     r \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m printer(obj)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    345\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 170\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(bytes_io, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\matplotlib\\backend_bases.py:2193\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2192\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2193\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2194\u001b[0m             filename,\n\u001b[0;32m   2195\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2196\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2197\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2198\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2199\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2200\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\matplotlib\\backend_bases.py:2043\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2041\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2042\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2043\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2046\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:497\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_pil(filename_or_obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m, pil_kwargs, metadata)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:446\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 446\u001b[0m mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[0;32m    447\u001b[0m     filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    448\u001b[0m     dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\matplotlib\\image.py:1656\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1654\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1655\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[1;32m-> 1656\u001b[0m image\u001b[38;5;241m.\u001b[39msave(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:2459\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2456\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2459\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n\u001b[0;32m   2460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   2461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\PIL\\PngImagePlugin.py:1412\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[0;32m   1409\u001b[0m         im, fp, chunk, rawmode, default_image, append_images\n\u001b[0;32m   1410\u001b[0m     )\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im:\n\u001b[1;32m-> 1412\u001b[0m     ImageFile\u001b[38;5;241m.\u001b[39m_save(im, _idat(fp, chunk), [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m im\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;241m0\u001b[39m, rawmode)])\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:540\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    538\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 540\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[38;5;28;01mNone\u001b[39;00m, exc)\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    542\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:559\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mencode(bufsize)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    560\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 디렉토리 경로 설정\n",
    "DATASET_PATH = 'Google_Recaptcha_V2_Images_Dataset/images/'\n",
    "LABELS_DIR = 'labels'\n",
    "MODEL_SAVE_PATH = 'conv2d_model.h5'\n",
    "AGENT_SAVE_PATH = 'ddqn_agent.h5'\n",
    "\n",
    "# 필요한 디렉토리가 없으면 생성\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "os.makedirs(LABELS_DIR, exist_ok=True)\n",
    "\n",
    "# 클래스 목록 (폴더 이름을 클래스명으로 가정)\n",
    "class_names = sorted([d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))])\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'orange', 'purple', 'pink', 'brown', 'gray']  # 색상 리스트\n",
    "\n",
    "# 데이터 라벨링 함수\n",
    "def label_image(image, img_name, classes, scale_factor=4):\n",
    "    \"\"\"\n",
    "    사용자가 이미지에 바운딩 박스와 클래스 라벨을 지정합니다.\n",
    "    image: 원본 이미지\n",
    "    img_name: 이미지 이름\n",
    "    classes: 클래스 리스트\n",
    "    scale_factor: 이미지 확대 배율\n",
    "    \"\"\"\n",
    "    orig_height, orig_width = image.shape[:2]\n",
    "    large_image = cv2.resize(image, (orig_width * scale_factor, orig_height * scale_factor))\n",
    "    \n",
    "    bboxes = []\n",
    "    while True:\n",
    "        # 사용자가 바운딩 박스를 선택\n",
    "        bbox = cv2.selectROI(f\"Bounding Box Selection - {img_name} (Press ESC to finish)\", large_image, fromCenter=False, showCrosshair=True)\n",
    "        if bbox[2] == 0 or bbox[3] == 0:  # ESC 키를 눌러 선택 종료\n",
    "            break\n",
    "        bboxes.append(bbox)\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    labels = []\n",
    "    for bbox in bboxes:\n",
    "        # 선택한 바운딩 박스 시각화\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(cv2.cvtColor(large_image, cv2.COLOR_BGR2RGB))\n",
    "        rect = Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], edgecolor='r', facecolor='none', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.title(f\"Selected Bounding Box for {img_name}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # 클래스 선택\n",
    "        print(\"Available Classes:\")\n",
    "        for i, cls in enumerate(classes):\n",
    "            print(f\"{i}: {cls}\")\n",
    "        while True:\n",
    "            class_input = input(\"Enter the class index for the selected bounding box (or type 'exit' to cancel): \")\n",
    "            if class_input.lower() == 'exit':\n",
    "                print(\"Labeling canceled.\")\n",
    "                return None\n",
    "            if class_input.isdigit() and int(class_input) in range(len(classes)):\n",
    "                class_idx = int(class_input)\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter a valid class index or 'exit'.\")\n",
    "        \n",
    "        # 바운딩 박스 좌표를 원본 이미지 크기로 변환\n",
    "        bbox_resized = (\n",
    "            bbox[0] // scale_factor,  # x 좌표\n",
    "            bbox[1] // scale_factor,  # y 좌표\n",
    "            bbox[2] // scale_factor,  # 폭 (width)\n",
    "            bbox[3] // scale_factor   # 높이 (height)\n",
    "        )\n",
    "        \n",
    "        labels.append({\n",
    "            \"class\": class_idx,\n",
    "            \"bbox\": bbox_resized\n",
    "        })\n",
    "        \n",
    "        # 선택한 바운딩 박스 시각화 (원본 이미지)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        color = colors[class_idx % len(colors)]\n",
    "        rect = Rectangle((bbox_resized[0], bbox_resized[1]), bbox_resized[2], bbox_resized[3], edgecolor=color, facecolor='none', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.title(f\"Class: {classes[class_idx]} - Verification\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# 라벨 데이터 저장 함수\n",
    "def save_labels(labels, img_name, save_dir=LABELS_DIR):\n",
    "    \"\"\"\n",
    "    라벨 데이터를 JSON 파일로 저장합니다.\n",
    "    labels: 바운딩 박스 리스트\n",
    "    img_name: 이미지 이름\n",
    "    save_dir: 라벨 파일을 저장할 디렉토리\n",
    "    \"\"\"\n",
    "    label_path = os.path.join(save_dir, f\"{os.path.splitext(img_name)[0]}.json\")\n",
    "    with open(label_path, 'w') as f:\n",
    "        json.dump(labels, f)\n",
    "    print(f\"Labels saved to {label_path}\")\n",
    "\n",
    "# 라벨 데이터 로드 함수\n",
    "def load_labels(img_name, labels_dir=LABELS_DIR):\n",
    "    label_path = os.path.join(labels_dir, f\"{os.path.splitext(img_name)[0]}.json\")\n",
    "    if not os.path.exists(label_path):\n",
    "        return None\n",
    "    with open(label_path, 'r') as f:\n",
    "        labels = json.load(f)\n",
    "    return labels\n",
    "\n",
    "# Conv2D 모델 정의\n",
    "class Conv2DModel:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=self.input_shape),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(4, activation='linear')  # (x, y, width, height)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def predict_bbox(self, image):\n",
    "        \"\"\"\n",
    "        이미지를 Conv2D 모델에 입력하여 바운딩 박스를 예측합니다.\n",
    "        \"\"\"\n",
    "        image_resized = cv2.resize(image, (self.input_shape[1], self.input_shape[0]))\n",
    "        image_normalized = image_resized / 255.0\n",
    "        image_expanded = np.expand_dims(image_normalized, axis=0)\n",
    "        return self.model.predict(image_expanded)[0]\n",
    "    \n",
    "    def train(self, X, y, epochs=10, batch_size=32, validation_split=0.2):\n",
    "        \"\"\"\n",
    "        Conv2D 모델을 훈련시킵니다.\n",
    "        \"\"\"\n",
    "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "# 강화학습 에이전트 정의 (DDQN)\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size, update_frequency=4):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # 할인율\n",
    "        self.epsilon = 1.0   # 탐험 비율 (exploration)\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        self.update_frequency = update_frequency  # 업데이트 주기\n",
    "        self.step_count = 0  # 스텝 카운트\n",
    "\n",
    "\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # 간단한 Neural Network 모델 구성\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # Target Model 업데이트 (DDQN 특징)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def act(self, state):\n",
    "        # 탐험 또는 최적 행동 선택\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # Q값이 최대인 행동 선택\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # 경험 저장\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        self.step_count += 1\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        # 예를 들어, 4 스텝마다 업데이트\n",
    "        if self.step_count % self.update_frequency != 0:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "# 강화학습 환경 정의 (HumanObjectDetectionEnv)\n",
    "class HumanObjectDetectionEnv:\n",
    "    def __init__(self, image, labels, target_class_idx, conv_model=None):\n",
    "        \"\"\"\n",
    "        강화학습 환경 초기화.\n",
    "        image: 입력 이미지\n",
    "        labels: 유저가 설정한 모든 바운딩박스 (여러 클래스 포함)\n",
    "        target_class_idx: 유저가 선택한 클래스 인덱스 (해당 클래스의 바운딩박스만 학습에 사용)\n",
    "        conv_model: Conv2D 모델 (초기 바운딩 박스를 예측하는 데 사용)\n",
    "        \"\"\"\n",
    "        self.image = image\n",
    "        self.image_height, self.image_width = image.shape[:2]  # 이미지 크기 자동 추출\n",
    "        self.ground_truth_bboxes = [bbox for bbox in labels if bbox['class'] == target_class_idx]  # 타겟 클래스의 바운딩 박스\n",
    "        self.target_class_idx = target_class_idx   # 타겟 클래스 인덱스\n",
    "        self.conv_model = conv_model  # Conv2D 모델\n",
    "        \n",
    "        self.state_size = 4  # 상태 크기 설정 (x, y, width, height)\n",
    "        \n",
    "        if self.conv_model:\n",
    "            # Conv2D 모델을 사용하여 초기 바운딩 박스 예측\n",
    "            self.current_bbox = self.conv_model.predict_bbox(self.image)\n",
    "            self.current_bbox = [int(coord) for coord in self.current_bbox]\n",
    "        else:\n",
    "            # 초기 랜덤 bounding box\n",
    "            self.current_bbox = self.random_bbox()\n",
    "        \n",
    "        self.prev_bbox = self.current_bbox.copy()  # 직전 바운딩박스를 저장\n",
    "        self.done = False\n",
    "        self.iou = self.calculate_max_iou(self.current_bbox)\n",
    "    \n",
    "    def random_bbox(self):\n",
    "        \"\"\"\n",
    "        랜덤 bounding box 생성 (픽셀 좌표로 생성).\n",
    "        \"\"\"\n",
    "        x = np.random.randint(10, self.image_width - 50)  # x 좌표 (10 ~ 이미지 너비 - 50 사이)\n",
    "        y = np.random.randint(10, self.image_height - 50)  # y 좌표 (10 ~ 이미지 높이 - 50 사이)\n",
    "        width = np.random.randint(30, min(100, self.image_width - x))  # 폭 크기 제한\n",
    "        height = np.random.randint(30, min(100, self.image_height - y))  # 높이 크기 제한\n",
    "        return [x, y, width, height]\n",
    "    \n",
    "    def calculate_iou(self, box1, box2):\n",
    "        \"\"\"\n",
    "        두 바운딩박스 간의 IoU 계산 (픽셀 좌표 사용).\n",
    "        \"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n",
    "        y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n",
    "        \n",
    "        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        box1_area = box1[2] * box1[3]\n",
    "        box2_area = box2[2] * box2[3]\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        \n",
    "        return inter_area / union_area if union_area > 0 else 0  # IoU 반환\n",
    "    \n",
    "    def calculate_max_iou(self, bbox):\n",
    "        \"\"\"\n",
    "        주어진 바운딩 박스와 타겟 클래스의 모든 바운딩 박스와의 최대 IoU를 계산합니다.\n",
    "        \"\"\"\n",
    "        max_iou = 0\n",
    "        for gt_bbox in self.ground_truth_bboxes:\n",
    "            iou = self.calculate_iou(bbox, gt_bbox['bbox'])\n",
    "            max_iou = max(max_iou, iou)\n",
    "        return max_iou\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        에이전트의 action에 따라 바운딩 박스를 조정하고 IoU 계산 (픽셀 좌표 기반).\n",
    "        action: 0 ~ 7 (왼쪽, 오른쪽, 위, 아래, 폭 넓히기, 폭 줄이기, 높이 넓히기, 높이 줄이기)\n",
    "        \"\"\"\n",
    "        # IoU가 0.5 미만이면 크게 움직이며 탐색, 그렇지 않으면 미세 조정\n",
    "        if self.iou < 0.5:\n",
    "            step_size = 10\n",
    "            size_change = 10\n",
    "        else:\n",
    "            step_size = 5\n",
    "            size_change = 5\n",
    "        \n",
    "        # Action에 따른 바운딩 박스 조정\n",
    "        if action == 0:   # 왼쪽 이동\n",
    "            self.current_bbox[0] = max(0, self.current_bbox[0] - step_size)\n",
    "        elif action == 1: # 오른쪽 이동\n",
    "            self.current_bbox[0] = min(self.image_width - self.current_bbox[2], self.current_bbox[0] + step_size)\n",
    "        elif action == 2: # 위로 이동\n",
    "            self.current_bbox[1] = max(0, self.current_bbox[1] - step_size)\n",
    "        elif action == 3: # 아래로 이동\n",
    "            self.current_bbox[1] = min(self.image_height - self.current_bbox[3], self.current_bbox[1] + step_size)\n",
    "        elif action == 4: # 폭 넓히기\n",
    "            self.current_bbox[2] = min(self.image_width - self.current_bbox[0], self.current_bbox[2] + size_change)\n",
    "        elif action == 5: # 폭 줄이기\n",
    "            self.current_bbox[2] = max(10, self.current_bbox[2] - size_change)\n",
    "        elif action == 6: # 높이 넓히기\n",
    "            self.current_bbox[3] = min(self.image_height - self.current_bbox[1], self.current_bbox[3] + size_change)\n",
    "        elif action == 7: # 높이 줄이기\n",
    "            self.current_bbox[3] = max(10, self.current_bbox[3] - size_change)\n",
    "        \n",
    "        # IoU 계산\n",
    "        max_iou = self.calculate_max_iou(self.current_bbox)\n",
    "        \n",
    "        # IoU가 줄어들면 직전 바운딩 박스로 복원\n",
    "        if max_iou < self.iou:\n",
    "            self.current_bbox = self.prev_bbox.copy()\n",
    "        else:\n",
    "            self.prev_bbox = self.current_bbox.copy()  # IoU가 개선되면 현재 바운딩 박스를 저장\n",
    "        \n",
    "        # IoU 업데이트\n",
    "        self.iou = max_iou\n",
    "        \n",
    "        # 목표 도달 여부 판단\n",
    "        if max_iou >= 0.9:  # IoU 0.9 이상이면 목표 도달\n",
    "            self.done = True\n",
    "            reward = 1.0\n",
    "            print(f\"목표 도달! IoU: {max_iou}\")\n",
    "        else:\n",
    "            self.done = False\n",
    "            reward = max_iou  # IoU를 보상으로 사용\n",
    "        \n",
    "        # bounding box 시각화 (환경 렌더링)\n",
    "        self.render()\n",
    "        \n",
    "        return np.array(self.current_bbox, dtype=np.float32), reward, self.done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        현재 에이전트의 바운딩박스와 유저가 설정한 바운딩박스를 시각적으로 렌더링.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1, figsize=(8,8))\n",
    "        ax.imshow(cv2.cvtColor(self.image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # 현재 에이전트의 bounding box 그리기\n",
    "        current_rect = Rectangle((self.current_bbox[0], \n",
    "                                  self.current_bbox[1]), \n",
    "                                 self.current_bbox[2], \n",
    "                                 self.current_bbox[3], \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(current_rect)\n",
    "\n",
    "        # 유저가 설정한 바운딩박스 (선택한 클래스에 해당하는 것만 그리기)\n",
    "        for gt_bbox in self.ground_truth_bboxes:\n",
    "            gt_rect = Rectangle((gt_bbox['bbox'][0], \n",
    "                                 gt_bbox['bbox'][1]), \n",
    "                                gt_bbox['bbox'][2], \n",
    "                                gt_bbox['bbox'][3], \n",
    "                                linewidth=2, edgecolor='g', facecolor='none')\n",
    "            ax.add_patch(gt_rect)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.title(f'현재 정확도:{self.iou}')\n",
    "        plt.show()\n",
    "\n",
    "def run_combined_model(image, conv2d_model, agent, env, class_idx, episodes=10, batch_size=32):\n",
    "    for e in range(episodes):\n",
    "        state = np.array(env.current_bbox, dtype=np.float32)\n",
    "        state = np.reshape(state, [1, agent.state_size])  # 상태 크기 조정\n",
    "        \n",
    "        for time in range(200):\n",
    "            # 에이전트가 행동 선택\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # 환경에 action 적용하고 다음 상태, 보상, 종료 여부 받기\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "            \n",
    "            # 에이전트에게 경험 저장\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # 상태 업데이트\n",
    "            clear_output(wait=True)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "                print(f\"Episode: {e+1}/{episodes}, Time: {time+1}, Reward: {reward}\")\n",
    "                break\n",
    "        \n",
    "        # 에피소드가 끝난 후에만 경험 재생\n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "        # 높은 IoU를 달성한 경우 Conv2D 모델 훈련\n",
    "        if env.iou >= 0.9:\n",
    "            print(f\"Episode: {e+1}/{episodes}, Time: {time+1}, IoU: {env.iou}, Conv2D 모델 학습\")\n",
    "            # Conv2D 모델 훈련을 위해 이미지와 현재 바운딩 박스 사용\n",
    "            image_resized = cv2.resize(image, (conv2d_model.input_shape[1], conv2d_model.input_shape[0]))\n",
    "            image_normalized = image_resized / 255.0\n",
    "            image_expanded = np.expand_dims(image_normalized, axis=0)\n",
    "            \n",
    "            # 바운딩 박스 좌표를 정규화\n",
    "            bbox = env.current_bbox\n",
    "            bbox_normalized = np.array(bbox, dtype=np.float32)\n",
    "            \n",
    "            # Conv2D 모델 훈련\n",
    "            conv2d_model.model.fit(image_expanded, np.expand_dims(bbox_normalized, axis=0), epochs=1, verbose=0)\n",
    "            break  # 학습 종료 후 다음 에피소드로 이동\n",
    "\n",
    "# 메인 루프\n",
    "def main():\n",
    "    # Conv2D 모델 초기화 (전체 데이터셋에 대해 하나의 모델 사용)\n",
    "    # 임의의 입력 이미지 크기로 Conv2D 모델 초기화 (실제 사용 시 적절히 설정)\n",
    "    sample_image_path = None\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(DATASET_PATH, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            sample_image_path = img_path\n",
    "            break\n",
    "        if sample_image_path:\n",
    "            break\n",
    "    \n",
    "    if not sample_image_path:\n",
    "        raise ValueError(\"데이터셋에 이미지가 없습니다.\")\n",
    "    \n",
    "    sample_image = cv2.imread(sample_image_path)\n",
    "    input_shape = (sample_image.shape[0], sample_image.shape[1], 3)\n",
    "    conv_model = Conv2DModel(input_shape)\n",
    "    \n",
    "    # RL 에이전트 초기화 (전체 데이터셋에 대해 하나의 에이전트 사용)\n",
    "    state_size = 4  # bounding box의 (x, y, width, height)\n",
    "    action_size = 8  # 8가지 행동 (왼쪽, 오른쪽, 위, 아래, 폭 넓히기, 폭 줄이기, 높이 넓히기, 높이 줄이기)\n",
    "    agent = DDQNAgent(state_size, action_size)\n",
    "    \n",
    "    # 데이터셋 순회 및 라벨링, 모델 훈련\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(DATASET_PATH, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"이미지를 로드할 수 없습니다: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Labeling image: {img_path}\")\n",
    "            labels = label_image(img, img_name, class_names)\n",
    "            if labels is None:\n",
    "                # 라벨링 중단 시 루프 종료\n",
    "                check_user = input('라벨링을 중단하시겠습니까? (no: 계속, other: 다음 이미지): ')\n",
    "                if check_user.lower() == 'no':\n",
    "                    print(\"라벨링 작업을 종료합니다.\")\n",
    "                    return\n",
    "                else:\n",
    "                    continue\n",
    "            # 라벨 데이터 저장\n",
    "            save_labels(labels, img_name, save_dir=LABELS_DIR)\n",
    "            \n",
    "            # 타겟 클래스별로 학습\n",
    "            target_classes = set([bbox['class'] for bbox in labels])\n",
    "            for target_class in target_classes:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Processing Class: {class_names[target_class]} in Image: {img_name}\")\n",
    "                env = HumanObjectDetectionEnv(img, labels, target_class, conv_model=conv_model)\n",
    "                \n",
    "                # RL 에이전트 학습\n",
    "                episodes = 10\n",
    "                run_combined_model(img, conv_model, agent, env, target_class, episodes=episodes, batch_size=32)\n",
    "                \n",
    "                # 시각화: 사용자, Conv2D, RL 에이전트의 바운딩 박스 표시\n",
    "                fig, ax = plt.subplots(1, figsize=(8,8))\n",
    "                ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "                \n",
    "                # 사용자 바운딩 박스\n",
    "                for bbox in labels:\n",
    "                    if bbox['class'] == target_class:\n",
    "                        rect = Rectangle((bbox['bbox'][0], bbox['bbox'][1]), bbox['bbox'][2], bbox['bbox'][3], linewidth=2, edgecolor='g', facecolor='none')\n",
    "                        ax.add_patch(rect)\n",
    "                \n",
    "                # Conv2D 예측 바운딩 박스\n",
    "                conv_bbox = conv_model.predict_bbox(img)\n",
    "                conv_bbox = [int(coord) for coord in conv_bbox]\n",
    "                rect = Rectangle((conv_bbox[0], conv_bbox[1]), conv_bbox[2], conv_bbox[3], linewidth=2, edgecolor='b', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # RL 에이전트 조정 바운딩 박스\n",
    "                rl_bbox = env.current_bbox\n",
    "                rect = Rectangle((rl_bbox[0], rl_bbox[1]), rl_bbox[2], rl_bbox[3], linewidth=2, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                plt.title(f\"Class: {class_names[target_class]} - User (Green), Conv2D (Blue), RL Agent (Red)\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "                # IoU 계산 및 결과 출력\n",
    "                iou = env.iou\n",
    "                print(f\"Final IoU for Class '{class_names[target_class]}': {iou}\")\n",
    "                \n",
    "                if iou >= 0.9:\n",
    "                    print(f\"IoU {iou} >= 0.9, 학습을 종료하고 다음 객체로 이동합니다.\\n\")\n",
    "                else:\n",
    "                    print(f\"IoU {iou} < 0.9, 추가 학습이 필요합니다.\\n\")\n",
    "    \n",
    "    # 최종 모델 저장\n",
    "    conv_model.model.save(MODEL_SAVE_PATH)\n",
    "    print(f\"Conv2D 모델이 {MODEL_SAVE_PATH}에 저장되었습니다.\")\n",
    "    \n",
    "    # DDQN 에이전트의 모델 저장 (옵션)\n",
    "    agent.model.save_weights(AGENT_SAVE_PATH)\n",
    "    agent.target_model.save_weights(AGENT_SAVE_PATH.replace('.h5', '_target.h5'))\n",
    "    print(f\"DDQN 에이전트의 모델이 {AGENT_SAVE_PATH}에 저장되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f147dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 강화학습 환경 정의 (HumanObjectDetectionEnv)\n",
    "class HumanObjectDetectionEnv:\n",
    "    def __init__(self, image, labels, target_index):\n",
    "        \"\"\"\n",
    "        강화학습 환경 초기화.\n",
    "        image: 입력 이미지\n",
    "        labels: 유저가 설정한 모든 바운딩박스 (여러 클래스 포함)\n",
    "        target_class_idx: 유저가 선택한 클래스 인덱스 (해당 클래스의 바운딩박스만 학습에 사용)\n",
    "        \"\"\"\n",
    "        self.image = image\n",
    "        self.image_height, self.image_width = image.shape[:2]  # 이미지 크기 자동 추출\n",
    "        self.ground_truth_bboxes = labels  # 실제 bounding box들 (유저가 선택한 여러 개)\n",
    "        self.target_class_idx = target_index   # 유저가 선택한 클래스에 해당하는 박스만 사용\n",
    "        self.current_bbox = self.random_bbox()  # 초기 랜덤 bounding box\n",
    "        self.prev_bbox = self.current_bbox.copy()  # 직전 바운딩박스를 저장\n",
    "        self.done = False\n",
    "        self.iou = 0  # 현재 IoU\n",
    "\n",
    "    def random_bbox(self):\n",
    "        \"\"\"\n",
    "        랜덤 bounding box 생성 (픽셀 좌표로 생성).\n",
    "        \"\"\"\n",
    "        x = np.random.randint(10, self.image_width - 50)  # x 좌표 (10 ~ 이미지 너비 - 50 사이)\n",
    "        y = np.random.randint(10, self.image_height - 50)  # y 좌표 (10 ~ 이미지 높이 - 50 사이)\n",
    "        width = np.random.randint(30, self.image_width - x)  # 폭 크기 제한\n",
    "        height = np.random.randint(30, self.image_height - y)  # 높이 크기 제한\n",
    "        return [x, y, width, height]\n",
    "\n",
    "    def calculate_iou(self, box1, box2):\n",
    "        \"\"\"\n",
    "        두 바운딩박스 간의 IoU 계산 (픽셀 좌표 사용).\n",
    "        \"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n",
    "        y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n",
    "        \n",
    "        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        box1_area = box1[2] * box1[3]\n",
    "        box2_area = box2[2] * box2[3]\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        \n",
    "        return inter_area / union_area  # IoU 반환\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        에이전트의 action에 따라 바운딩박스를 조정하고 IoU 계산 (픽셀 좌표 기반).\n",
    "        action: 0 ~ 7 (위치와 크기 조정)\n",
    "        \"\"\"\n",
    "        # IoU가 0이면 크게 움직이며 탐색\n",
    "        if self.iou == 0:\n",
    "            if action == 0:   # 왼쪽 이동\n",
    "                self.current_bbox[0] = max(0, self.current_bbox[0] - 10)\n",
    "            elif action == 1: # 오른쪽 이동\n",
    "                self.current_bbox[0] = min(self.image_width - self.current_bbox[2], self.current_bbox[0] + 10)\n",
    "            elif action == 2: # 위로 이동\n",
    "                self.current_bbox[1] = max(0, self.current_bbox[1] - 10)\n",
    "            elif action == 3: # 아래로 이동\n",
    "                self.current_bbox[1] = min(self.image_height - self.current_bbox[3], self.current_bbox[1] + 10)\n",
    "        elif self.iou !=0:\n",
    "            temp_iou = self.iou\n",
    "            # IoU가 0이 아닐 때는 미세한 움직임과 크기 조정으로 최적화\n",
    "            if action == 0:   # 왼쪽 이동\n",
    "                self.current_bbox[0] = max(0, self.current_bbox[0] - 5)\n",
    "            elif action == 1: # 오른쪽 이동\n",
    "                self.current_bbox[0] = min(self.image_width - self.current_bbox[2], self.current_bbox[0] + 5)\n",
    "            elif action == 2: # 위로 이동\n",
    "                self.current_bbox[1] = max(0, self.current_bbox[1] - 5)\n",
    "            elif action == 3: # 아래로 이동\n",
    "                self.current_bbox[1] = min(self.image_height - self.current_bbox[3], self.current_bbox[1] + 5)\n",
    "            elif action == 4: # 폭 넓히기\n",
    "                self.current_bbox[2] = min(self.image_width - self.current_bbox[0], self.current_bbox[2] + 5)\n",
    "            elif action == 5: # 폭 줄이기\n",
    "                self.current_bbox[2] = max(10, self.current_bbox[2] - 5)\n",
    "            elif action == 6: # 높이 넓히기\n",
    "                self.current_bbox[3] = min(self.image_height - self.current_bbox[1], self.current_bbox[3] + 5)\n",
    "            elif action == 7: # 높이 줄이기\n",
    "                self.current_bbox[3] = max(10, self.current_bbox[3] - 5)\n",
    "\n",
    "        # 선택한 클래스에 해당하는 ground truth 박스들과 IoU 계산\n",
    "        max_iou = 0\n",
    "        for gt_bbox in self.ground_truth_bboxes:\n",
    "            if gt_bbox['class'] == self.target_class_idx:\n",
    "                iou = self.calculate_iou(self.current_bbox, gt_bbox['bbox'])\n",
    "                max_iou = max(max_iou, iou)\n",
    "\n",
    "        # IoU가 줄어들면 직전 바운딩박스로 복원\n",
    "        if max_iou < self.iou:\n",
    "            self.current_bbox = self.prev_bbox.copy()\n",
    "        else:\n",
    "            self.prev_bbox = self.current_bbox.copy()  # IoU가 개선되면 현재 바운딩박스를 저장\n",
    "\n",
    "        # IoU 업데이트\n",
    "        self.iou = max_iou\n",
    "\n",
    "        # 목표 도달 여부 판단\n",
    "        if max_iou > 0.9:  # IoU 0.9 이상이면 목표 도달\n",
    "            self.done = True\n",
    "            print(f\"목표 도달! IoU: {max_iou}\")\n",
    "        else:\n",
    "            print(f\"현재 IoU: {max_iou}\")\n",
    "        \n",
    "        # bounding box 시각화 (환경 렌더링)\n",
    "        self.render()\n",
    "\n",
    "        return self.current_bbox, max_iou, self.done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        현재 에이전트의 바운딩박스와 유저가 설정한 바운딩박스를 시각적으로 렌더링.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(self.image)\n",
    "\n",
    "        # 현재 에이전트의 bounding box 그리기\n",
    "        current_rect = Rectangle((self.current_bbox[0], \n",
    "                                  self.current_bbox[1]), \n",
    "                                 self.current_bbox[2], \n",
    "                                 self.current_bbox[3], \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(current_rect)\n",
    "\n",
    "        # 유저가 설정한 바운딩박스 (선택한 클래스에 해당하는 것만 그리기)\n",
    "        for gt_bbox in self.ground_truth_bboxes:\n",
    "            if gt_bbox['class'] == self.target_class_idx:\n",
    "                gt_rect = Rectangle((gt_bbox['bbox'][0], \n",
    "                                     gt_bbox['bbox'][1]), \n",
    "                                    gt_bbox['bbox'][2], \n",
    "                                    gt_bbox['bbox'][3], \n",
    "                                    linewidth=2, edgecolor='g', facecolor='none')\n",
    "                ax.add_patch(gt_rect)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# 강화학습 에이전트 정의 (DDQN 등)\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # 할인율\n",
    "        self.epsilon = 1.0   # 탐험 비율 (exploration)\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # 간단한 Neural Network 모델 구성\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Target Model 업데이트 (DDQN 특징)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        # 탐험 또는 최적 행동 선택\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # Q값이 최대인 행동 선택\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # 경험 저장\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # 메모리에서 샘플 추출하여 학습\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# 실행\n",
    "\n",
    "state_size = 4  # 예시로, state_size는 바운딩 박스 정보 (x, y, width, height)를 가정\n",
    "action_size = 8  # 8가지 행동 (위, 아래, 왼쪽, 오른쪽, 폭 확대/축소, 높이 확대/축소)\n",
    "\n",
    "agent = DDQNAgent(state_size, action_size)\n",
    "\n",
    "agent.model.load_weights('ddqn_model_d.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 저장된 모델 불러오기\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 모델 불러오기\n",
    "model = load_model('ddqn_model_only.keras')\n",
    "image = cv2.imread(\"C:/Users/humming/Downloads/0925/Google_Recaptcha_V2_Images_Dataset/images/Bicycle/Bicycle (671).png\")\n",
    "# 새로운 이미지를 예측하는 예시\n",
    "state = np.array([image])  # 이미지로부터 상태 생성\n",
    "plt.imshow(state[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87156eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 이미지 불러오기\n",
    "new_image = cv2.imread(\"C:/Users/humming/Downloads/0925/Google_Recaptcha_V2_Images_Dataset/images/Bicycle/Bicycle (671).png\")\n",
    "target_class_idx=0\n",
    "if new_image is None:\n",
    "    print(\"이미지를 불러오지 못했습니다. 경로를 확인하세요.\")\n",
    "else:\n",
    "    print(\"이미지를 성공적으로 불러왔습니다.\")\n",
    "# 새로운 이미지를 기반으로 환경 초기화 (바운딩 박스 라벨은 없다고 가정)\n",
    "env = HumanObjectDetectionEnv(new_image, [], target_class_idx)\n",
    "\n",
    "# 상태 초기화\n",
    "state = env.random_bbox()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "\n",
    "# 학습된 모델로 행동 선택\n",
    "action = agent.act(state)\n",
    "\n",
    "# 환경에 적용하여 바운딩 박스 최적화\n",
    "next_state, reward, done = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd99efb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot convert '4' to a shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 316\u001b[0m\n\u001b[0;32m    314\u001b[0m state_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# bounding box의 (x, y, width, height)\u001b[39;00m\n\u001b[0;32m    315\u001b[0m action_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m  \u001b[38;5;66;03m# 8가지 행동 (위, 아래, 왼쪽, 오른쪽, 폭 확대/축소, 높이 확대/축소)\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDQNAgent(state_size, action_size)\n\u001b[0;32m    318\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n",
      "Cell \u001b[1;32mIn[4], line 153\u001b[0m, in \u001b[0;36mDDQNAgent.__init__\u001b[1;34m(self, input_shape, action_size)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.995\u001b[39m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_model()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_model()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target_model()\n",
      "Cell \u001b[1;32mIn[4], line 164\u001b[0m, in \u001b[0;36mDDQNAgent._build_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    161\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Conv2D 레이어 추가 (이미지 특징 추출)\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape))\n\u001b[0;32m    165\u001b[0m model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMaxPooling2D(pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m    167\u001b[0m model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\sequential.py:87\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer, rebuild)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_input_shape_arg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(InputLayer(shape\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39m_input_shape_arg))\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# If we are passed a Keras tensor created by keras.Input(), we\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# extract the input layer from its keras history and use that.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_history\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:47\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, shape, batch_size, dtype, sparse, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must pass a `shape` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mstandardize_shape(shape)\n\u001b[0;32m     48\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m (batch_size,) \u001b[38;5;241m+\u001b[39m shape\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mstandardize_shape(batch_shape)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:515\u001b[0m, in \u001b[0;36mstandardize_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndefined shapes are not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(shape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__iter__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to a shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, tf\u001b[38;5;241m.\u001b[39mTensorShape):\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;66;03m# `tf.TensorShape` may contain `Dimension` objects.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m         \u001b[38;5;66;03m# We need to convert the items in it to either int or `None`\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert '4' to a shape."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 강화학습 환경 정의 (HumanObjectDetectionEnv)\n",
    "class HumanObjectDetectionEnv:\n",
    "    def __init__(self, image, labels, target_index):\n",
    "        \"\"\"\n",
    "        강화학습 환경 초기화.\n",
    "        image: 입력 이미지\n",
    "        labels: 유저가 설정한 모든 바운딩박스 (여러 클래스 포함)\n",
    "        target_class_idx: 유저가 선택한 클래스 인덱스 (해당 클래스의 바운딩박스만 학습에 사용)\n",
    "        \"\"\"\n",
    "        self.image = image\n",
    "        self.image_height, self.image_width = image.shape[:2]  # 이미지 크기 자동 추출\n",
    "        self.ground_truth_bboxes = labels  # 실제 bounding box들 (유저가 선택한 여러 개)\n",
    "        self.target_class_idx = target_index   # 유저가 선택한 클래스에 해당하는 박스만 사용\n",
    "        self.current_bbox = self.random_bbox()  # 초기 랜덤 bounding box\n",
    "        self.prev_bbox = self.current_bbox.copy()  # 직전 바운딩박스를 저장\n",
    "        self.done = False\n",
    "        self.iou = 0  # 현재 IoU\n",
    "\n",
    "    def random_bbox(self):\n",
    "        \"\"\"\n",
    "        랜덤 bounding box 생성 (픽셀 좌표로 생성).\n",
    "        \"\"\"\n",
    "        x = np.random.randint(10, self.image_width - 50)  # x 좌표 (10 ~ 이미지 너비 - 50 사이)\n",
    "        y = np.random.randint(10, self.image_height - 50)  # y 좌표 (10 ~ 이미지 높이 - 50 사이)\n",
    "        width = np.random.randint(30, self.image_width - x)  # 폭 크기 제한\n",
    "        height = np.random.randint(30, self.image_height - y)  # 높이 크기 제한\n",
    "        return [x, y, width, height]\n",
    "\n",
    "    def calculate_iou(self, box1, box2):\n",
    "        \"\"\"\n",
    "        두 바운딩박스 간의 IoU 계산 (픽셀 좌표 사용).\n",
    "        \"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[0] + box1[2], box2[0] + box2[2])\n",
    "        y2 = min(box1[1] + box1[3], box2[1] + box2[3])\n",
    "        \n",
    "        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        box1_area = box1[2] * box1[3]\n",
    "        box2_area = box2[2] * box2[3]\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        \n",
    "        return inter_area / union_area  # IoU 반환\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        에이전트의 action에 따라 바운딩박스를 조정하고 IoU 계산 (픽셀 좌표 기반).\n",
    "        action: 0 ~ 7 (위치와 크기 조정)\n",
    "        \"\"\"\n",
    "        # IoU가 0이면 크게 움직이며 탐색\n",
    "        if self.iou == 0:\n",
    "            if action == 0:   # 왼쪽 이동\n",
    "                self.current_bbox[0] = max(0, self.current_bbox[0] - 10)\n",
    "            elif action == 1: # 오른쪽 이동\n",
    "                self.current_bbox[0] = min(self.image_width - self.current_bbox[2], self.current_bbox[0] + 10)\n",
    "            elif action == 2: # 위로 이동\n",
    "                self.current_bbox[1] = max(0, self.current_bbox[1] - 10)\n",
    "            elif action == 3: # 아래로 이동\n",
    "                self.current_bbox[1] = min(self.image_height - self.current_bbox[3], self.current_bbox[1] + 10)\n",
    "        else:\n",
    "            # IoU가 0이 아닐 때는 미세한 움직임과 크기 조정으로 최적화\n",
    "            if action == 0:   # 왼쪽 이동\n",
    "                self.current_bbox[0] = max(0, self.current_bbox[0] - 5)\n",
    "            elif action == 1: # 오른쪽 이동\n",
    "                self.current_bbox[0] = min(self.image_width - self.current_bbox[2], self.current_bbox[0] + 5)\n",
    "            elif action == 2: # 위로 이동\n",
    "                self.current_bbox[1] = max(0, self.current_bbox[1] - 5)\n",
    "            elif action == 3: # 아래로 이동\n",
    "                self.current_bbox[1] = min(self.image_height - self.current_bbox[3], self.current_bbox[1] + 5)\n",
    "            elif action == 4: # 폭 넓히기\n",
    "                self.current_bbox[2] = min(self.image_width - self.current_bbox[0], self.current_bbox[2] + 5)\n",
    "            elif action == 5: # 폭 줄이기\n",
    "                self.current_bbox[2] = max(10, self.current_bbox[2] - 5)\n",
    "            elif action == 6: # 높이 넓히기\n",
    "                self.current_bbox[3] = min(self.image_height - self.current_bbox[1], self.current_bbox[3] + 5)\n",
    "            elif action == 7: # 높이 줄이기\n",
    "                self.current_bbox[3] = max(10, self.current_bbox[3] - 5)\n",
    "\n",
    "        # 선택한 클래스에 해당하는 ground truth 박스들과 IoU 계산\n",
    "        max_iou = 0\n",
    "        for gt_bbox in self.ground_truth_bboxes:\n",
    "            if gt_bbox['class'] == self.target_class_idx:\n",
    "                iou = self.calculate_iou(self.current_bbox, gt_bbox['bbox'])\n",
    "                max_iou = max(max_iou, iou)\n",
    "\n",
    "        # IoU가 줄어들면 직전 바운딩박스로 복원\n",
    "        if max_iou < self.iou:\n",
    "            self.current_bbox = self.prev_bbox.copy()\n",
    "        else:\n",
    "            self.prev_bbox = self.current_bbox.copy()  # IoU가 개선되면 현재 바운딩박스를 저장\n",
    "\n",
    "        # IoU 업데이트\n",
    "        self.iou = max_iou\n",
    "\n",
    "        # 목표 도달 여부 판단\n",
    "        if max_iou > 0.9:  # IoU 0.9 이상이면 목표 도달\n",
    "            self.done = True\n",
    "            print(f\"목표 도달! IoU: {max_iou}\")\n",
    "        else:\n",
    "            print(f\"현재 IoU: {max_iou}\")\n",
    "        \n",
    "        # bounding box 시각화 (환경 렌더링)\n",
    "        self.render()\n",
    "\n",
    "        return self.current_bbox, max_iou, self.done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        현재 에이전트의 바운딩박스와 유저가 설정한 바운딩박스를 시각적으로 렌더링.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(self.image)\n",
    "\n",
    "        # 현재 에이전트의 bounding box 그리기\n",
    "        current_rect = Rectangle((self.current_bbox[0], \n",
    "                                  self.current_bbox[1]), \n",
    "                                 self.current_bbox[2], \n",
    "                                 self.current_bbox[3], \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(current_rect)\n",
    "\n",
    "        # 유저가 설정한 바운딩박스 (선택한 클래스에 해당하는 것만 그리기)\n",
    "        for gt_bbox in self.ground_truth_bboxes:\n",
    "            if gt_bbox['class'] == self.target_class_idx:\n",
    "                gt_rect = Rectangle((gt_bbox['bbox'][0], \n",
    "                                     gt_bbox['bbox'][1]), \n",
    "                                    gt_bbox['bbox'][2], \n",
    "                                    gt_bbox['bbox'][3], \n",
    "                                    linewidth=2, edgecolor='g', facecolor='none')\n",
    "                ax.add_patch(gt_rect)\n",
    "\n",
    "        plt.show()\n",
    "class DDQNAgent:\n",
    "    def __init__(self, input_shape, action_size):\n",
    "        self.input_shape = input_shape\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # 할인율\n",
    "        self.epsilon = 1.0   # 탐험 비율 (exploration)\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Conv2D를 사용한 강화학습 모델.\n",
    "        \"\"\"\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        # Conv2D 레이어 추가 (이미지 특징 추출)\n",
    "        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=self.input_shape))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        # Flatten 레이어를 통해 1차원으로 변환\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "        # Dense 레이어로 fully connected network 연결\n",
    "        model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "\n",
    "        # 액션 수 만큼의 출력 (linear activation)\n",
    "        model.add(tf.keras.layers.Dense(self.action_size, activation='linear'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Target Model 업데이트\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        # 탐험 또는 최적 행동 선택\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = np.expand_dims(state, axis=0)  # 상태의 배치 차원 추가\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # 경험 저장\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # 메모리에서 샘플 추출하여 학습\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATASET_PATH = 'Google_Recaptcha_V2_Images_Dataset/images/'\n",
    "class_names = sorted(os.listdir(DATASET_PATH))\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'orange', 'purple', 'pink', 'brown', 'gray']  # 'tab20' 컬러맵에서 12개의 색상을 추출\n",
    "\n",
    "# 클래스 목록 (사용자가 정의할 수 있음)\n",
    "classes = class_names\n",
    "X_train = []\n",
    "y_train = []  # 이미지별로 라벨을 딕셔너리 형태로 저장할 리스트\n",
    "\n",
    "# 확대 배율 (라벨링할 때 이미지를 크게 보이도록 설정)\n",
    "SCALE_FACTOR = 4  # 예: 4배 확대\n",
    "\n",
    "# 이미지를 확대하여 표시하고, bounding box 좌표를 원본 이미지에 맞게 조정하는 함수\n",
    "def label_image(image):\n",
    "    orig_height, orig_width = image.shape[:2]\n",
    "    large_image = cv2.resize(image, (orig_width * SCALE_FACTOR, orig_height * SCALE_FACTOR))\n",
    "\n",
    "    # OpenCV로 bounding box 지정 (사용자가 마우스로 선택)\n",
    "    bboxes = []\n",
    "    while True:\n",
    "        bbox = cv2.selectROI(f\"bounding box{img_name} (ESC key = end)\", large_image, fromCenter=False, showCrosshair=True)\n",
    "        if bbox[2] == 0 or bbox[3] == 0:  # ESC 키를 눌러 선택을 종료한 경우\n",
    "            break\n",
    "        bboxes.append(bbox)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    labels = []\n",
    "    for bbox in bboxes:\n",
    "        for i, cls in enumerate(classes):\n",
    "            print(f\"{i}: {cls}\")\n",
    "        plt.imshow(large_image)\n",
    "        rect = plt.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], edgecolor='r', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.title(f\"check your select{img_name}\")\n",
    "\n",
    "        plt.show()\n",
    "        print(f\"클래스 이름: {class_names}\")\n",
    "        print(\"클래스를 선택하세요: \")\n",
    "        class_idx = input(\"클래스 인덱스를 입력하세요 (중단하려면 'exit' 입력): \")\n",
    "        clear_output() \n",
    "        if class_idx == 'exit':\n",
    "            print(\"라벨링 작업을 중단합니다.\")\n",
    "            return None  # 작업 중단\n",
    "\n",
    "        class_idx = int(class_idx)\n",
    "        \n",
    "        # Bounding box 좌표를 원본 크기로 변환\n",
    "        bbox_resized = (\n",
    "            bbox[0] // SCALE_FACTOR,  # x 좌표\n",
    "            bbox[1] // SCALE_FACTOR,  # y 좌표\n",
    "            bbox[2] // SCALE_FACTOR,  # 폭 (width)\n",
    "            bbox[3] // SCALE_FACTOR   # 높이 (height)\n",
    "        )\n",
    "        \n",
    "        # Bounding box 좌표를 원본 이미지 크기로 정규화\n",
    "        bbox_normalized = {\n",
    "            \"class\": class_idx,\n",
    "            \"bbox\": [\n",
    "                bbox_resized[0],  # x 좌표 (정규화)\n",
    "                bbox_resized[1],  # y 좌표 (정규화)\n",
    "                bbox_resized[2],  # 폭 (정규화)\n",
    "                bbox_resized[3]  # 높이 (정규화)\n",
    "            ]\n",
    "        }\n",
    "        labels.append(bbox_normalized)\n",
    "        plt.imshow(image)\n",
    "        color = colors[class_idx % len(colors)]\n",
    "        rect = plt.Rectangle((bbox_normalized['bbox'][0], bbox_normalized['bbox'][1]), \n",
    "                             bbox_normalized['bbox'][2], bbox_normalized['bbox'][3], \n",
    "                             edgecolor=color, facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.title(f\"class: {classes[class_idx]} - check\")\n",
    "        plt.show()\n",
    "    return labels\n",
    "\n",
    "# 이미지 폴더 내의 모든 이미지에 대해 라벨링 작업 수행\n",
    "for label, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(DATASET_PATH, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        # 이미지 읽기\n",
    "        clear_output() \n",
    "        img = cv2.imread(img_path)\n",
    "        print(class_names)\n",
    "        if img is None:\n",
    "            continue  # 이미지 로드 실패 시 건너뛰기\n",
    "        labels = label_image(img)  # 여러 bounding box 라벨링\n",
    "        if labels == None:\n",
    "            check_user = input('더 할꺼임? no치면 작업 완전 종료 other 치면 다른 디렉토리로 이동')\n",
    "            if check_user == 'no':\n",
    "                check_next ='no'\n",
    "                break\n",
    "            elif check_user == 'other':\n",
    "                check_next = 'other'\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        target_class=int(input('학습할 클래스의 인덱스를 입력:'))\n",
    "        env = HumanObjectDetectionEnv(img, labels, target_class)\n",
    "\n",
    "        # 강화학습 에이전트 생성 및 학습 과정\n",
    "        state_size = 4  # bounding box의 (x, y, width, height)\n",
    "        action_size = 8  # 8가지 행동 (위, 아래, 왼쪽, 오른쪽, 폭 확대/축소, 높이 확대/축소)\n",
    "        agent = DDQNAgent(state_size, action_size)\n",
    "\n",
    "        episodes = 10\n",
    "        for e in range(episodes):\n",
    "            state = env.random_bbox()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            \n",
    "            for time in range(200):\n",
    "                # 에이전트가 행동 선택\n",
    "                action = agent.act(state)\n",
    "                \n",
    "                # 환경에 action 적용하고 다음 상태, 보상, 종료 여부 받기\n",
    "                next_state, reward, done = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                \n",
    "                # 에이전트에게 경험 저장\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                \n",
    "                # 상태 업데이트\n",
    "                state = next_state\n",
    "                clear_output(wait=True)\n",
    "                if done:\n",
    "                    agent.update_target_model()\n",
    "                    print(f\"Episode: {e}/{episodes}, Score: {reward}\")\n",
    "                    break\n",
    "                \n",
    "            if len(agent.memory) > 32:\n",
    "                agent.replay(32)\n",
    "    if check_next == 'no':\n",
    "        break\n",
    "    elif check_next =='other':\n",
    "        continue\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0ff9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 392\u001b[0m\n\u001b[0;32m    389\u001b[0m env \u001b[38;5;241m=\u001b[39m HumanObjectDetectionEnv(img, labels)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 392\u001b[0m run_combined_model(img, conv_model, agent, env, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 337\u001b[0m, in \u001b[0;36mrun_combined_model\u001b[1;34m(image, conv2d_model, agent, env, episodes, batch_size)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m--> 337\u001b[0m     agent\u001b[38;5;241m.\u001b[39mreplay(batch_size)\n",
      "Cell \u001b[1;32mIn[2], line 292\u001b[0m, in \u001b[0;36mDDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    290\u001b[0m minibatch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory, batch_size)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, action, reward, next_state, done \u001b[38;5;129;01min\u001b[39;00m minibatch:\n\u001b[1;32m--> 292\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(state)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    294\u001b[0m         target[\u001b[38;5;241m0\u001b[39m][action] \u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:450\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ):\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    451\u001b[0m         x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    452\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    453\u001b[0m         steps_per_epoch\u001b[38;5;241m=\u001b[39msteps,\n\u001b[0;32m    454\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    455\u001b[0m         distribute_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    456\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution,\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:668\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m--> 668\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[0;32m    670\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[0;32m    671\u001b[0m         dataset\n\u001b[0;32m    672\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:677\u001b[0m, in \u001b[0;36mTFEpochIterator._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_adapter\u001b[38;5;241m.\u001b[39mget_tf_dataset()\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:140\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# performance.\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(permutation)\u001b[38;5;241m.\u001b[39mprefetch(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_batch_indices\u001b[39m(indices):\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        A Dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2311\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[0;32m   2308\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2309\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[1;32m-> 2311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m map_op\u001b[38;5;241m.\u001b[39m_map_v2(\n\u001b[0;32m   2312\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2313\u001b[0m     map_func,\n\u001b[0;32m   2314\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mnum_parallel_calls,\n\u001b[0;32m   2315\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[0;32m   2316\u001b[0m     name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m     34\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[0;32m     35\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _MapDataset(\n\u001b[0;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[0;32m     41\u001b[0m       input_dataset,\n\u001b[0;32m     42\u001b[0m       map_func,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:107\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m structured_function\u001b[38;5;241m.\u001b[39mStructuredFunctionWrapper(\n\u001b[0;32m    108\u001b[0m     map_func,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformation_name(),\n\u001b[0;32m    110\u001b[0m     dataset\u001b[38;5;241m=\u001b[39minput_dataset,\n\u001b[0;32m    111\u001b[0m     use_legacy_function\u001b[38;5;241m=\u001b[39muse_legacy_function)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m    113\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmap_dataset(\n\u001b[0;32m    114\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m fn_factory()\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1251\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1250\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_concrete_function_garbage_collected(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1252\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1221\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1219\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1220\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(args, kwargs, add_initializers_to\u001b[38;5;241m=\u001b[39minitializers)\n\u001b[0;32m   1222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m   1225\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[0;32m    697\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    698\u001b[0m )\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[0;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[0;32m    180\u001b[0m   )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[0;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[0;32m    285\u001b[0m )\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[0;32m    313\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    314\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    316\u001b[0m     func_graph\u001b[38;5;241m=\u001b[39mfunc_graph,\n\u001b[0;32m    317\u001b[0m     add_control_dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m disable_acd,\n\u001b[0;32m    318\u001b[0m     arg_names\u001b[38;5;241m=\u001b[39mfunction_type_utils\u001b[38;5;241m.\u001b[39mto_arg_names(function_type),\n\u001b[0;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m )\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1064\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n\u001b[1;32m-> 1064\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m   1065\u001b[0m     convert, func_outputs, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# flatten and unflatten func_args and func_kwargs to maintain parity\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;66;03m# from flattening which sorts by key\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m func_args \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   1070\u001b[0m     func_args,\n\u001b[0;32m   1071\u001b[0m     nest\u001b[38;5;241m.\u001b[39mflatten(func_args, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   1072\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest.py:628\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    544\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest_util\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    629\u001b[0m       nest_util\u001b[38;5;241m.\u001b[39mModality\u001b[38;5;241m.\u001b[39mCORE, func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    630\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py:1065\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[0;32m    969\u001b[0m \n\u001b[0;32m    970\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[1;32m-> 1065\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[0;32m   1067\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\nest_util.py:1105\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m   1101\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[0;32m   1104\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m-> 1105\u001b[0m     [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m   1106\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[0;32m   1107\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1055\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.convert\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be compatible with tf.function, Python functions \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust return zero or more Tensors or ExtensionTypes or None \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1051\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues; in compilation of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(python_func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found return \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1052\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, which is not a Tensor or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1053\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtensionType.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_control_dependencies:\n\u001b[1;32m-> 1055\u001b[0m   x \u001b[38;5;241m=\u001b[39m deps_ctx\u001b[38;5;241m.\u001b[39mmark_as_return(x)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py:246\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.mark_as_return\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tensor_array_ops\u001b[38;5;241m.\u001b[39mbuild_ta_with_new_flow(tensor, flow)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# We want to make the return values depend on the stateful operations, but\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# we don't want to introduce a cycle, so we make the return value the result\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# of a new identity operation that the stateful operations definitely don't\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# depend on.\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m tensor \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(tensor)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returned_tensors\u001b[38;5;241m.\u001b[39madd(tensor)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:310\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    307\u001b[0m   \u001b[38;5;66;03m# Make sure we get an input with handle data attached from resource\u001b[39;00m\n\u001b[0;32m    308\u001b[0m   \u001b[38;5;66;03m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m   \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m--> 310\u001b[0m ret \u001b[38;5;241m=\u001b[39m gen_array_ops\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;28minput\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# Propagate handle data for happier shape inference for resource variables.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_handle_data\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:5009\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   5007\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m   5008\u001b[0m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m-> 5009\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m _op_def_library\u001b[38;5;241m.\u001b[39m_apply_op_helper(\n\u001b[0;32m   5010\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   5011\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[0;32m   5012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:796\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    791\u001b[0m must_colocate_inputs \u001b[38;5;241m=\u001b[39m [val \u001b[38;5;28;01mfor\u001b[39;00m arg, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(op_def\u001b[38;5;241m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    792\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mis_ref]\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    794\u001b[0m   \u001b[38;5;66;03m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    795\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 796\u001b[0m   op \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39m_create_op_internal(op_type_name, inputs, dtypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    797\u001b[0m                              name\u001b[38;5;241m=\u001b[39mscope, input_types\u001b[38;5;241m=\u001b[39minput_types,\n\u001b[0;32m    798\u001b[0m                              attrs\u001b[38;5;241m=\u001b[39mattr_protos, op_def\u001b[38;5;241m=\u001b[39mop_def)\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# for more details.\u001b[39;00m\n\u001b[0;32m    804\u001b[0m outputs \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39moutputs\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_create_op_internal(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2682\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2679\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2682\u001b[0m   ret \u001b[38;5;241m=\u001b[39m Operation\u001b[38;5;241m.\u001b[39mfrom_node_def(\n\u001b[0;32m   2683\u001b[0m       node_def,\n\u001b[0;32m   2684\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2685\u001b[0m       inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m   2686\u001b[0m       output_types\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[0;32m   2687\u001b[0m       control_inputs\u001b[38;5;241m=\u001b[39mcontrol_inputs,\n\u001b[0;32m   2688\u001b[0m       input_types\u001b[38;5;241m=\u001b[39minput_types,\n\u001b[0;32m   2689\u001b[0m       original_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_original_op,\n\u001b[0;32m   2690\u001b[0m       op_def\u001b[38;5;241m=\u001b[39mop_def,\n\u001b[0;32m   2691\u001b[0m   )\n\u001b[0;32m   2692\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1177\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1174\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1177\u001b[0m c_op \u001b[38;5;241m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[38;5;241m=\u001b[39mop_def)\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\humming\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1034\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1030\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(name),\n\u001b[0;32m   1031\u001b[0m                                          serialized)\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1034\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1036\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_policy_network(state_size, action_size):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(state_size,)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(action_size, activation='softmax')  # 각 행동에 대한 확률 출력\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "# 예시 사용\n",
    "state_size = images.shape[1] * images.shape[2] * images.shape[3]  # 평탄화된 이미지 벡터 크기\n",
    "action_size = num_classes  # 예: 클래스 수 만큼의 행동\n",
    "\n",
    "policy_model = build_policy_network(state_size, action_size)\n",
    "policy_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "221d65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def train_policy_gradient(env, model, episodes=1000, gamma=0.99):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "        with tf.GradientTape() as tape:\n",
    "            state_input = tf.expand_dims(state, 0)  # 배치 차원 추가\n",
    "            action_probs = model(state_input, training=True)\n",
    "            action = np.random.choice(action_size, p=action_probs.numpy()[0])\n",
    "            one_hot_action = tf.one_hot(action, action_size)\n",
    "            # 에이전트의 행동을 환경에 적용\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # 손실 정의: -log(probability of action) * reward\n",
    "            loss = -tf.math.log(action_probs[0][action] + 1e-10) * reward\n",
    "        # 그래디언트 계산 및 업데이트\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}/{episodes} - Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 초기화\n",
    "env = ObjectDetectionEnv(images, labels, class_names, IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "# 강화학습 학습 실행\n",
    "train_policy_gradient(env, policy_model, episodes=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
