{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "financial = pd.read_excel(\"C:/Users/humming/Downloads/상장폐지 종목 예측 프로젝트/학습데이터/financial_train.xlsx\")\n",
    "financial\n",
    "non_financial = pd.read_excel(\"C:/Users/humming/Downloads/상장폐지 종목 예측 프로젝트/학습데이터/non_financial_train.xlsx\")\n",
    "non_financial\n",
    "train = pd.concat([financial.drop('레이블', axis=1), non_financial.drop('종목코드', axis=1)], axis=1)\n",
    "test = pd.read_excel(\"C:/Users/humming/Downloads/상장폐지 종목 예측 프로젝트/테스트데이터/test.xlsx\")\n",
    "# Feature and label separation\n",
    "X_train = train.drop('레이블',axis=1)\n",
    "y_train = train['레이블']\n",
    "X_test = test.drop('레이블',axis=1)\n",
    "y_test = test['레이블']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['종목코드', '총자산', '총자본회전율', '부채비율', '매출액증가율', '자기자본회전율', '당기순이익', '최대주주변경',\n",
       "       '대표이사변경', '전환사채', '파산신청', '거래정지', '불성실공시법인', '레이블'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['종목코드', '총자산', '총자본회전율', '부채비율', '매출액증가율', '자기자본회전율', '당기순이익', '최대주주변경',\n",
       "       '대표이사변경', '전환사채', '파산신청', '거래정지', '불성실공시법인'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Index(['최대주주변경', '대표이사변경', '파산신청', '거래정지', '불성실공시법인'], dtype='object')\n",
      "Random Forest \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9737    0.9867        76\n",
      "           1     0.9231    1.0000    0.9600        24\n",
      "\n",
      "    accuracy                         0.9800       100\n",
      "   macro avg     0.9615    0.9868    0.9733       100\n",
      "weighted avg     0.9815    0.9800    0.9803       100\n",
      "\n",
      "Gradient Boosting Index(['총자산', '총자본회전율', '매출액증가율', '파산신청', '거래정지'], dtype='object')\n",
      "Gradient Boosting \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9737    0.9737    0.9737        76\n",
      "           1     0.9167    0.9167    0.9167        24\n",
      "\n",
      "    accuracy                         0.9600       100\n",
      "   macro avg     0.9452    0.9452    0.9452       100\n",
      "weighted avg     0.9600    0.9600    0.9600       100\n",
      "\n",
      "SVM Index(['부채비율', '자기자본회전율', '대표이사변경', '파산신청', '거래정지'], dtype='object')\n",
      "SVM \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9867    0.9737    0.9801        76\n",
      "           1     0.9200    0.9583    0.9388        24\n",
      "\n",
      "    accuracy                         0.9700       100\n",
      "   macro avg     0.9533    0.9660    0.9595       100\n",
      "weighted avg     0.9707    0.9700    0.9702       100\n",
      "\n",
      "Neural Network Index(['총자산', '매출액증가율', '당기순이익', '전환사채', '거래정지'], dtype='object')\n",
      "Neural Network \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9867    0.9737    0.9801        76\n",
      "           1     0.9200    0.9583    0.9388        24\n",
      "\n",
      "    accuracy                         0.9700       100\n",
      "   macro avg     0.9533    0.9660    0.9595       100\n",
      "weighted avg     0.9707    0.9700    0.9702       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "financial = pd.read_excel(\"C:/Users/humming/Downloads/상장폐지 종목 예측 프로젝트/학습데이터/financial_train.xlsx\")\n",
    "financial\n",
    "non_financial = pd.read_excel(\"C:/Users/humming/Downloads/상장폐지 종목 예측 프로젝트/학습데이터/non_financial_train.xlsx\")\n",
    "non_financial\n",
    "train = pd.concat([financial.drop('레이블', axis=1), non_financial.drop('종목코드', axis=1)], axis=1)\n",
    "test = pd.read_excel(\"C:/Users/humming/Downloads/상장폐지 종목 예측 프로젝트/테스트데이터/test.xlsx\")\n",
    "# Feature and label separation\n",
    "X_train = train.drop('레이블',axis=1)\n",
    "y_train = train['레이블']\n",
    "X_test = test.drop('레이블',axis=1)\n",
    "y_test = test['레이블']\n",
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "\n",
    "# Scale the features for models that are sensitive to feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "#     'Logistic Regression':LogisticRegression(\n",
    "#     penalty='l2',              # 규제 유형: 'l1', 'l2', 'elasticnet', 'none'\n",
    "#     C=1,                     # 규제 강도: 값이 작을수록 강한 규제\n",
    "#     solver='newton-cg',            # 최적화 알고리즘: 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'\n",
    "#     max_iter=1000,              # 최대 반복 횟수\n",
    "#     random_state=40            # 난수 시드\n",
    "# ),\n",
    "    'Random Forest':RandomForestClassifier(\n",
    "    # # n_estimators=100,           # 트리의 개수\n",
    "    # # max_depth=None,             # 트리의 최대 깊이\n",
    "    # min_samples_split=2,        # 노드를 분할하기 위한 최소 샘플 수\n",
    "    # min_samples_leaf=1,         # 리프 노드가 되기 위한 최소 샘플 수\n",
    "    # #max_features='auto',        # 각 트리에서 고려할 최대 피처 수\n",
    "    # bootstrap=True,             # 부트스트랩 샘플링 여부\n",
    "    random_state=40           # 난수 시드\n",
    "),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "    # # learning_rate=0.1,           # 학습률\n",
    "    # # n_estimators=100,            # 부스팅 단계의 개수\n",
    "    # max_depth=3,                 # 개별 결정 트리의 최대 깊이\n",
    "    # min_samples_split=2,         # 노드를 분할하기 위한 최소 샘플 수\n",
    "    # min_samples_leaf=1,          # 리프 노드가 되기 위한 최소 샘플 수\n",
    "    # subsample=1.0,               # 각 트리에 사용할 샘플의 비율\n",
    "    # max_features=None,           # 각 트리에서 고려할 최대 피처 수\n",
    "    random_state=40              # 난수 시드\n",
    "),\n",
    "    'SVM': SVC(\n",
    "    # C=1.0,                      # 규제 강도\n",
    "    # kernel='rbf',               # 커널 종류: 'linear', 'poly', 'rbf', 'sigmoid'\n",
    "    # degree=3,                   # 다항 커널일 때의 차수\n",
    "    # gamma='scale',              # 'scale', 'auto' 또는 float 값\n",
    "    # coef0=0.0,                  # 다항 및 시그모이드 커널의 독립 항\n",
    "    # probability=False,          # 확률 추정 수행 여부\n",
    "    random_state=40           # 난수 시드\n",
    "),\n",
    "    'Neural Network': MLPClassifier(\n",
    "    # hidden_layer_sizes=(100,),   # 은닉층의 뉴런 개수와 은닉층 개수\n",
    "    # activation='relu',           # 활성화 함수: 'identity', 'logistic', 'tanh', 'relu'\n",
    "    # # solver='adam',               # 최적화 알고리즘: 'lbfgs', 'sgd', 'adam'\n",
    "    # alpha=0.001,                # L2 규제 매개변수\n",
    "    # batch_size='auto',           # 배치 크기\n",
    "    # learning_rate='constant',    # 학습률 일정 방식: 'constant', 'invscaling', 'adaptive'\n",
    "    max_iter=1000,                # 최대 반복 횟수\n",
    "    random_state=40              # 난수 시드\n",
    ")\n",
    "}\n",
    "\n",
    "\n",
    "# 선택된 피처 출력\n",
    "\n",
    "# Evaluate models using cross-validation and display the results\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for SVM and Neural Network\n",
    "    if name in ['SVM', 'Neural Network']:\n",
    "        sfs = SequentialFeatureSelector(model, n_features_to_select=5, direction='forward', cv=5)\n",
    "        sfs.fit(X_train_scaled, y_train)\n",
    "        selected_features = X_train.columns[sfs.get_support()]\n",
    "        print(name,selected_features)\n",
    "        model.fit(X_train_scaled[:,sfs.get_support()], y_train)\n",
    "        y_pred = model.predict(X_test_scaled[:,sfs.get_support()])\n",
    "        #print(selected_features)\n",
    "    else:\n",
    "        sfs = SequentialFeatureSelector(model, n_features_to_select=5, direction='forward', cv=5)\n",
    "        sfs.fit(X_train, y_train)\n",
    "        selected_features = X_train.columns[sfs.get_support()]\n",
    "        print(name,selected_features)\n",
    "        model.fit(X_train[selected_features], y_train)\n",
    "        y_pred = model.predict(X_test[selected_features])\n",
    "     \n",
    "    # Calculate the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(name,'\\n',classification_report(y_test, y_pred, digits=4))\n",
    "    l=[]\n",
    "    for i in range(3):\n",
    "        l.append(float(classification_report(y_test, y_pred, digits=4).split('\\n')[-3].split('     ')[1].split('    ')[i]))\n",
    "    sum(l)/3\n",
    "    # Store the results\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "        \n",
    "    }\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df.sort_values(by='Accuracy', ascending=False, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(3):\n",
    "    l.append(float(classification_report(y_test, y_pred, digits=4).split('\\n')[-3].split('     ')[1].split('    ')[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.9738666666666668 40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9617333333333334"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.9738666666666668\n",
      "0.9738666666666668 1\n",
      "2\n",
      "0.9738666666666668\n",
      "0.9738666666666668 2\n",
      "3\n",
      "0.9738666666666668\n",
      "0.9738666666666668 3\n",
      "4\n",
      "0.9738666666666668\n",
      "0.9738666666666668 4\n",
      "5\n",
      "0.9617333333333334\n",
      "6\n",
      "0.9617333333333334\n",
      "7\n",
      "0.9617333333333334\n",
      "8\n",
      "0.9617333333333334\n",
      "9\n",
      "0.9617333333333334\n",
      "10\n",
      "0.9617333333333334\n",
      "11\n",
      "0.9738666666666668\n",
      "0.9738666666666668 11\n",
      "12\n",
      "0.9617333333333334\n",
      "13\n",
      "0.9617333333333334\n",
      "14\n",
      "0.9617333333333334\n",
      "15\n",
      "0.9617333333333334\n",
      "16\n",
      "0.9617333333333334\n",
      "17\n",
      "0.9617333333333334\n",
      "18\n",
      "0.9617333333333334\n",
      "19\n",
      "0.9738666666666668\n",
      "0.9738666666666668 19\n",
      "20\n",
      "0.9738666666666668\n",
      "0.9738666666666668 20\n",
      "21\n",
      "0.9617333333333334\n",
      "22\n",
      "0.9617333333333334\n",
      "23\n",
      "0.9617333333333334\n",
      "24\n",
      "0.9738666666666668\n",
      "0.9738666666666668 24\n",
      "25\n",
      "0.9738666666666668\n",
      "0.9738666666666668 25\n",
      "26\n",
      "0.9617333333333334\n",
      "27\n",
      "0.9617333333333334\n",
      "28\n",
      "0.9617333333333334\n",
      "29\n",
      "0.9738666666666668\n",
      "0.9738666666666668 29\n",
      "30\n",
      "0.9617333333333334\n",
      "31\n",
      "0.9738666666666668\n",
      "0.9738666666666668 31\n",
      "32\n",
      "0.9617333333333334\n",
      "33\n",
      "0.9617333333333334\n",
      "34\n",
      "0.9617333333333334\n",
      "35\n",
      "0.9617333333333334\n",
      "36\n",
      "0.9738666666666668\n",
      "0.9738666666666668 36\n",
      "37\n",
      "0.9738666666666668\n",
      "0.9738666666666668 37\n",
      "38\n",
      "0.9617333333333334\n",
      "39\n",
      "0.9617333333333334\n",
      "40\n",
      "0.9738666666666668\n",
      "0.9738666666666668 40\n",
      "41\n",
      "0.9617333333333334\n",
      "42\n",
      "0.9617333333333334\n",
      "43\n",
      "0.9738666666666668\n",
      "0.9738666666666668 43\n",
      "44\n",
      "0.9738666666666668\n",
      "0.9738666666666668 44\n",
      "45\n",
      "0.9617333333333334\n",
      "46\n",
      "0.9617333333333334\n",
      "47\n",
      "0.9617333333333334\n",
      "48\n",
      "0.9738666666666668\n",
      "0.9738666666666668 48\n",
      "49\n",
      "0.9738666666666668\n",
      "0.9738666666666668 49\n",
      "50\n",
      "0.9617333333333334\n",
      "51\n",
      "0.9738666666666668\n",
      "0.9738666666666668 51\n",
      "52\n",
      "0.9738666666666668\n",
      "0.9738666666666668 52\n",
      "53\n",
      "0.9617333333333334\n",
      "54\n",
      "0.9738666666666668\n",
      "0.9738666666666668 54\n",
      "55\n",
      "0.9617333333333334\n",
      "56\n",
      "0.9738666666666668\n",
      "0.9738666666666668 56\n",
      "57\n",
      "0.9738666666666668\n",
      "0.9738666666666668 57\n",
      "58\n",
      "0.9617333333333334\n",
      "59\n",
      "0.9617333333333334\n",
      "60\n",
      "0.9617333333333334\n",
      "61\n",
      "0.9617333333333334\n",
      "62\n",
      "0.9617333333333334\n",
      "63\n",
      "0.9617333333333334\n",
      "64\n",
      "0.9617333333333334\n",
      "65\n",
      "0.9617333333333334\n",
      "66\n",
      "0.9617333333333334\n",
      "67\n",
      "0.9617333333333334\n",
      "68\n",
      "0.9738666666666668\n",
      "0.9738666666666668 68\n",
      "69\n",
      "0.9617333333333334\n",
      "70\n",
      "0.9738666666666668\n",
      "0.9738666666666668 70\n",
      "71\n",
      "0.9617333333333334\n",
      "72\n",
      "0.9738666666666668\n",
      "0.9738666666666668 72\n",
      "73\n",
      "0.9617333333333334\n",
      "74\n",
      "0.9617333333333334\n",
      "75\n",
      "0.9617333333333334\n",
      "76\n",
      "0.9738666666666668\n",
      "0.9738666666666668 76\n",
      "77\n",
      "0.9617333333333334\n",
      "78\n",
      "0.9617333333333334\n",
      "79\n",
      "0.9617333333333334\n",
      "80\n",
      "0.9617333333333334\n",
      "81\n",
      "0.9617333333333334\n",
      "82\n",
      "0.9738666666666668\n",
      "0.9738666666666668 82\n",
      "83\n",
      "0.9617333333333334\n",
      "84\n",
      "0.9738666666666668\n",
      "0.9738666666666668 84\n",
      "85\n",
      "0.9617333333333334\n",
      "86\n",
      "0.9738666666666668\n",
      "0.9738666666666668 86\n",
      "87\n",
      "0.9617333333333334\n",
      "88\n",
      "0.9617333333333334\n",
      "89\n",
      "0.9617333333333334\n",
      "90\n",
      "0.9738666666666668\n",
      "0.9738666666666668 90\n",
      "91\n",
      "0.9617333333333334\n",
      "92\n",
      "0.9738666666666668\n",
      "0.9738666666666668 92\n",
      "93\n",
      "0.9738666666666668\n",
      "0.9738666666666668 93\n",
      "94\n",
      "0.9617333333333334\n",
      "95\n",
      "0.9617333333333334\n",
      "96\n",
      "0.9738666666666668\n",
      "0.9738666666666668 96\n",
      "97\n",
      "0.9617333333333334\n",
      "98\n",
      "0.9617333333333334\n",
      "99\n",
      "0.9617333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "best_best = 0.0\n",
    "best_model_state = 0\n",
    "for i in range(1,100):\n",
    "    print(i)\n",
    "    rft = RandomForestClassifier(random_state=i)\n",
    "    rft.fit(X_train[['최대주주변경', '대표이사변경', '파산신청', '거래정지', '불성실공시법인']], y_train)\n",
    "    y_pred = rft.predict(X_test[['최대주주변경', '대표이사변경', '파산신청', '거래정지', '불성실공시법인']])\n",
    "\n",
    "    l=[]\n",
    "    for j in range(3):\n",
    "        l.append(float(classification_report(y_test, y_pred, digits=4).split('\\n')[-3].split('     ')[1].split('    ')[j]))\n",
    "    best = sum(l)/3\n",
    "    print(best)\n",
    "    if best >=best_best:\n",
    "        best_best = best\n",
    "        \n",
    "        best_model_state = i\n",
    "        print(best_best, best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9738666666666668"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 순차적 피처 선택기 정의\n",
    "sfs = SequentialFeatureSelector(RandomForestClassifier(), n_features_to_select=5, direction='forward', cv=5)\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# 선택된 피처 출력\n",
    "selected_features = X.columns[sfs.get_support()]\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ddd\n",
      "Epoch [1/1000], Loss: 0.7069, Accuracy: 8.00%\n",
      "Epoch [2/1000], Loss: 0.4497, Accuracy: 15.38%\n",
      "Epoch [3/1000], Loss: 0.3474, Accuracy: 57.14%\n",
      "Epoch [4/1000], Loss: 0.2405, Accuracy: 61.11%\n",
      "Epoch [5/1000], Loss: 0.4412, Accuracy: 71.79%\n",
      "Epoch [6/1000], Loss: 0.1951, Accuracy: 71.79%\n",
      "Epoch [7/1000], Loss: 0.1485, Accuracy: 70.00%\n",
      "Epoch [8/1000], Loss: 0.5135, Accuracy: 73.91%\n",
      "Epoch [9/1000], Loss: 0.1380, Accuracy: 74.42%\n",
      "Epoch [10/1000], Loss: 0.5499, Accuracy: 73.17%\n",
      "Epoch [11/1000], Loss: 0.2361, Accuracy: 78.26%\n",
      "Epoch [12/1000], Loss: 0.3971, Accuracy: 71.79%\n",
      "Epoch [13/1000], Loss: 0.1012, Accuracy: 73.91%\n",
      "Epoch [14/1000], Loss: 0.1745, Accuracy: 73.17%\n",
      "Epoch [15/1000], Loss: 0.0614, Accuracy: 74.42%\n",
      "Epoch [16/1000], Loss: 0.4197, Accuracy: 72.73%\n",
      "Epoch [17/1000], Loss: 0.1365, Accuracy: 71.43%\n",
      "Epoch [18/1000], Loss: 0.3252, Accuracy: 78.26%\n",
      "Epoch [19/1000], Loss: 0.5091, Accuracy: 72.73%\n",
      "Epoch [20/1000], Loss: 0.1589, Accuracy: 75.00%\n",
      "Epoch [21/1000], Loss: 0.4266, Accuracy: 74.42%\n",
      "Epoch [22/1000], Loss: 0.5431, Accuracy: 72.73%\n",
      "Epoch [23/1000], Loss: 0.1191, Accuracy: 73.68%\n",
      "Epoch [24/1000], Loss: 0.3786, Accuracy: 72.73%\n",
      "Epoch [25/1000], Loss: 0.1190, Accuracy: 72.73%\n",
      "Epoch [26/1000], Loss: 0.3698, Accuracy: 71.11%\n",
      "Epoch [27/1000], Loss: 0.8263, Accuracy: 72.73%\n",
      "Epoch [28/1000], Loss: 0.9136, Accuracy: 76.19%\n",
      "Epoch [29/1000], Loss: 0.1100, Accuracy: 65.00%\n",
      "Epoch [30/1000], Loss: 0.3097, Accuracy: 71.43%\n",
      "Epoch [31/1000], Loss: 0.1249, Accuracy: 73.91%\n",
      "Epoch [32/1000], Loss: 0.1423, Accuracy: 73.17%\n",
      "Epoch [33/1000], Loss: 0.4876, Accuracy: 75.00%\n",
      "Epoch [34/1000], Loss: 0.1702, Accuracy: 73.17%\n",
      "Epoch [35/1000], Loss: 0.4565, Accuracy: 71.11%\n",
      "Epoch [36/1000], Loss: 0.0625, Accuracy: 71.11%\n",
      "Epoch [37/1000], Loss: 0.2832, Accuracy: 71.11%\n",
      "Epoch [38/1000], Loss: 0.0725, Accuracy: 73.91%\n",
      "Epoch [39/1000], Loss: 0.1115, Accuracy: 76.19%\n",
      "Epoch [40/1000], Loss: 0.7811, Accuracy: 71.11%\n",
      "Epoch [41/1000], Loss: 0.0387, Accuracy: 72.73%\n",
      "Epoch [42/1000], Loss: 0.6380, Accuracy: 72.73%\n",
      "Epoch [43/1000], Loss: 1.1622, Accuracy: 76.60%\n",
      "Epoch [44/1000], Loss: 0.2525, Accuracy: 79.07%\n",
      "Epoch [45/1000], Loss: 0.3319, Accuracy: 81.82%\n",
      "Epoch [46/1000], Loss: 0.0311, Accuracy: 80.00%\n",
      "Epoch [47/1000], Loss: 0.0583, Accuracy: 74.42%\n",
      "Epoch [48/1000], Loss: 0.9132, Accuracy: 73.47%\n",
      "Epoch [49/1000], Loss: 0.5783, Accuracy: 72.34%\n",
      "Epoch [50/1000], Loss: 0.1306, Accuracy: 76.19%\n",
      "Epoch [51/1000], Loss: 0.2530, Accuracy: 72.73%\n",
      "Epoch [52/1000], Loss: 0.1081, Accuracy: 75.00%\n",
      "Epoch [53/1000], Loss: 0.7232, Accuracy: 73.17%\n",
      "Epoch [54/1000], Loss: 0.0370, Accuracy: 72.34%\n",
      "Epoch [55/1000], Loss: 0.2360, Accuracy: 73.91%\n",
      "Epoch [56/1000], Loss: 0.3439, Accuracy: 79.17%\n",
      "Epoch [57/1000], Loss: 0.0930, Accuracy: 75.56%\n",
      "Epoch [58/1000], Loss: 0.1120, Accuracy: 71.43%\n",
      "Epoch [59/1000], Loss: 0.1131, Accuracy: 73.91%\n",
      "Epoch [60/1000], Loss: 0.6953, Accuracy: 73.47%\n",
      "Epoch [61/1000], Loss: 0.6059, Accuracy: 73.91%\n",
      "Epoch [62/1000], Loss: 0.0331, Accuracy: 74.42%\n",
      "Epoch [63/1000], Loss: 0.1943, Accuracy: 73.91%\n",
      "Epoch [64/1000], Loss: 0.2743, Accuracy: 76.60%\n",
      "Epoch [65/1000], Loss: 0.1305, Accuracy: 75.56%\n",
      "Epoch [66/1000], Loss: 0.0611, Accuracy: 77.27%\n",
      "Epoch [67/1000], Loss: 0.0571, Accuracy: 76.60%\n",
      "Epoch [68/1000], Loss: 0.0514, Accuracy: 76.60%\n",
      "Epoch [69/1000], Loss: 0.5735, Accuracy: 76.60%\n",
      "Epoch [70/1000], Loss: 0.1315, Accuracy: 76.19%\n",
      "Epoch [71/1000], Loss: 0.0329, Accuracy: 73.17%\n",
      "Epoch [72/1000], Loss: 0.3667, Accuracy: 80.00%\n",
      "Epoch [73/1000], Loss: 1.6422, Accuracy: 86.96%\n",
      "Epoch [74/1000], Loss: 0.0447, Accuracy: 84.00%\n",
      "Epoch [75/1000], Loss: 0.6109, Accuracy: 73.17%\n",
      "Epoch [76/1000], Loss: 0.2378, Accuracy: 78.43%\n",
      "Epoch [77/1000], Loss: 0.2157, Accuracy: 84.44%\n",
      "Epoch [78/1000], Loss: 0.1417, Accuracy: 77.27%\n",
      "Epoch [79/1000], Loss: 0.0668, Accuracy: 75.56%\n",
      "Epoch [80/1000], Loss: 0.2496, Accuracy: 76.60%\n",
      "Epoch [81/1000], Loss: 0.7095, Accuracy: 77.27%\n",
      "Epoch [82/1000], Loss: 0.3451, Accuracy: 68.42%\n",
      "Epoch [83/1000], Loss: 0.3847, Accuracy: 81.48%\n",
      "Epoch [84/1000], Loss: 0.1289, Accuracy: 72.73%\n",
      "Epoch [85/1000], Loss: 0.0520, Accuracy: 72.73%\n",
      "Epoch [86/1000], Loss: 0.4871, Accuracy: 76.60%\n",
      "Epoch [87/1000], Loss: 0.3134, Accuracy: 74.42%\n",
      "Epoch [88/1000], Loss: 0.3006, Accuracy: 74.51%\n",
      "Epoch [89/1000], Loss: 0.0748, Accuracy: 74.42%\n",
      "Epoch [90/1000], Loss: 0.7954, Accuracy: 74.42%\n",
      "Epoch [91/1000], Loss: 0.5233, Accuracy: 85.71%\n",
      "Epoch [92/1000], Loss: 0.1458, Accuracy: 84.44%\n",
      "Epoch [93/1000], Loss: 0.0498, Accuracy: 77.27%\n",
      "Epoch [94/1000], Loss: 0.0909, Accuracy: 80.00%\n",
      "Epoch [95/1000], Loss: 0.0298, Accuracy: 77.27%\n",
      "Epoch [96/1000], Loss: 0.0502, Accuracy: 77.55%\n",
      "Epoch [97/1000], Loss: 0.1630, Accuracy: 80.00%\n",
      "Epoch [98/1000], Loss: 0.4660, Accuracy: 83.33%\n",
      "Epoch [99/1000], Loss: 0.0456, Accuracy: 75.56%\n",
      "Epoch [100/1000], Loss: 0.0169, Accuracy: 73.17%\n",
      "Epoch [101/1000], Loss: 0.0717, Accuracy: 77.55%\n",
      "Epoch [102/1000], Loss: 0.0562, Accuracy: 78.26%\n",
      "Epoch [103/1000], Loss: 0.4421, Accuracy: 79.17%\n",
      "Epoch [104/1000], Loss: 0.3453, Accuracy: 72.73%\n",
      "Epoch [105/1000], Loss: 0.1301, Accuracy: 80.00%\n",
      "Epoch [106/1000], Loss: 0.1335, Accuracy: 80.85%\n",
      "Epoch [107/1000], Loss: 0.3320, Accuracy: 80.85%\n",
      "Epoch [108/1000], Loss: 0.2034, Accuracy: 83.33%\n",
      "Epoch [109/1000], Loss: 0.2055, Accuracy: 75.56%\n",
      "Epoch [110/1000], Loss: 0.0480, Accuracy: 76.60%\n",
      "Epoch [111/1000], Loss: 0.0949, Accuracy: 72.34%\n",
      "Epoch [112/1000], Loss: 0.0320, Accuracy: 75.00%\n",
      "Epoch [113/1000], Loss: 0.1180, Accuracy: 77.27%\n",
      "Epoch [114/1000], Loss: 0.3771, Accuracy: 80.00%\n",
      "Epoch [115/1000], Loss: 0.0786, Accuracy: 77.27%\n",
      "Epoch [116/1000], Loss: 0.3028, Accuracy: 80.00%\n",
      "Epoch [117/1000], Loss: 0.5669, Accuracy: 82.61%\n",
      "Epoch [118/1000], Loss: 0.3032, Accuracy: 80.00%\n",
      "Epoch [119/1000], Loss: 0.1691, Accuracy: 77.27%\n",
      "Epoch [120/1000], Loss: 0.5143, Accuracy: 72.34%\n",
      "Epoch [121/1000], Loss: 0.0076, Accuracy: 69.77%\n",
      "Epoch [122/1000], Loss: 0.0959, Accuracy: 78.43%\n",
      "Epoch [123/1000], Loss: 0.2829, Accuracy: 80.00%\n",
      "Epoch [124/1000], Loss: 0.0703, Accuracy: 85.11%\n",
      "Epoch [125/1000], Loss: 0.0281, Accuracy: 78.43%\n",
      "Epoch [126/1000], Loss: 0.3804, Accuracy: 78.26%\n",
      "Epoch [127/1000], Loss: 0.0535, Accuracy: 79.17%\n",
      "Epoch [128/1000], Loss: 0.1387, Accuracy: 80.85%\n",
      "Epoch [129/1000], Loss: 0.3955, Accuracy: 83.02%\n",
      "Epoch [130/1000], Loss: 0.6001, Accuracy: 80.85%\n",
      "Epoch [131/1000], Loss: 1.3798, Accuracy: 78.26%\n",
      "Epoch [132/1000], Loss: 0.3641, Accuracy: 87.50%\n",
      "Epoch [133/1000], Loss: 0.0996, Accuracy: 76.19%\n",
      "Epoch [134/1000], Loss: 0.2490, Accuracy: 83.33%\n",
      "Epoch [135/1000], Loss: 0.2439, Accuracy: 76.19%\n",
      "Epoch [136/1000], Loss: 0.1162, Accuracy: 82.61%\n",
      "Epoch [137/1000], Loss: 0.0188, Accuracy: 79.17%\n",
      "Epoch [138/1000], Loss: 0.1476, Accuracy: 82.61%\n",
      "Epoch [139/1000], Loss: 0.0583, Accuracy: 82.61%\n",
      "Epoch [140/1000], Loss: 0.3409, Accuracy: 80.00%\n",
      "Epoch [141/1000], Loss: 0.0290, Accuracy: 82.61%\n",
      "Epoch [142/1000], Loss: 0.3462, Accuracy: 80.77%\n",
      "Epoch [143/1000], Loss: 0.7206, Accuracy: 78.26%\n",
      "Epoch [144/1000], Loss: 0.0548, Accuracy: 72.73%\n",
      "Epoch [145/1000], Loss: 0.0910, Accuracy: 78.43%\n",
      "Epoch [146/1000], Loss: 0.0189, Accuracy: 74.42%\n",
      "Epoch [147/1000], Loss: 0.4113, Accuracy: 80.00%\n",
      "Epoch [148/1000], Loss: 0.1428, Accuracy: 77.55%\n",
      "Epoch [149/1000], Loss: 0.0351, Accuracy: 80.85%\n",
      "Epoch [150/1000], Loss: 0.1669, Accuracy: 79.17%\n",
      "Epoch [151/1000], Loss: 0.1451, Accuracy: 83.33%\n",
      "Epoch [152/1000], Loss: 0.0924, Accuracy: 81.63%\n",
      "Epoch [153/1000], Loss: 0.0694, Accuracy: 78.26%\n",
      "Epoch [154/1000], Loss: 0.0976, Accuracy: 81.63%\n",
      "Epoch [155/1000], Loss: 0.0287, Accuracy: 78.26%\n",
      "Epoch [156/1000], Loss: 0.0817, Accuracy: 83.33%\n",
      "Epoch [157/1000], Loss: 0.2406, Accuracy: 80.85%\n",
      "Epoch [158/1000], Loss: 0.0330, Accuracy: 80.00%\n",
      "Epoch [159/1000], Loss: 0.5321, Accuracy: 80.00%\n",
      "Epoch [160/1000], Loss: 0.4094, Accuracy: 78.26%\n",
      "Epoch [161/1000], Loss: 0.1500, Accuracy: 76.19%\n",
      "Epoch [162/1000], Loss: 0.1081, Accuracy: 82.35%\n",
      "Epoch [163/1000], Loss: 0.3375, Accuracy: 82.61%\n",
      "Epoch [164/1000], Loss: 0.6416, Accuracy: 77.27%\n",
      "Epoch [165/1000], Loss: 0.0575, Accuracy: 75.47%\n",
      "Epoch [166/1000], Loss: 0.0222, Accuracy: 76.19%\n",
      "Epoch [167/1000], Loss: 0.3663, Accuracy: 79.25%\n",
      "Epoch [168/1000], Loss: 0.0134, Accuracy: 80.85%\n",
      "Epoch [169/1000], Loss: 0.3581, Accuracy: 82.61%\n",
      "Epoch [170/1000], Loss: 0.0259, Accuracy: 80.77%\n",
      "Epoch [171/1000], Loss: 0.0146, Accuracy: 81.82%\n",
      "Epoch [172/1000], Loss: 0.1121, Accuracy: 78.43%\n",
      "Epoch [173/1000], Loss: 0.3377, Accuracy: 79.17%\n",
      "Epoch [174/1000], Loss: 0.3837, Accuracy: 76.92%\n",
      "Epoch [175/1000], Loss: 0.0377, Accuracy: 81.82%\n",
      "Epoch [176/1000], Loss: 0.1426, Accuracy: 84.44%\n",
      "Epoch [177/1000], Loss: 0.0735, Accuracy: 79.17%\n",
      "Epoch [178/1000], Loss: 0.0803, Accuracy: 80.85%\n",
      "Epoch [179/1000], Loss: 0.0653, Accuracy: 78.26%\n",
      "Epoch [180/1000], Loss: 1.5630, Accuracy: 83.02%\n",
      "Epoch [181/1000], Loss: 0.0397, Accuracy: 81.82%\n",
      "Epoch [182/1000], Loss: 0.1648, Accuracy: 83.33%\n",
      "Epoch [183/1000], Loss: 0.6758, Accuracy: 83.33%\n",
      "Epoch [184/1000], Loss: 0.1245, Accuracy: 80.85%\n",
      "Epoch [185/1000], Loss: 0.0510, Accuracy: 78.26%\n",
      "Epoch [186/1000], Loss: 1.2748, Accuracy: 77.55%\n",
      "Epoch [187/1000], Loss: 0.2211, Accuracy: 83.33%\n",
      "Epoch [188/1000], Loss: 0.5433, Accuracy: 78.26%\n",
      "Epoch [189/1000], Loss: 0.0792, Accuracy: 81.63%\n",
      "Epoch [190/1000], Loss: 0.2040, Accuracy: 77.27%\n",
      "Epoch [191/1000], Loss: 0.0854, Accuracy: 79.17%\n",
      "Epoch [192/1000], Loss: 0.0164, Accuracy: 77.55%\n",
      "Epoch [193/1000], Loss: 0.1351, Accuracy: 79.17%\n",
      "Epoch [194/1000], Loss: 0.1055, Accuracy: 79.17%\n",
      "Epoch [195/1000], Loss: 0.2080, Accuracy: 80.85%\n",
      "Epoch [196/1000], Loss: 0.1134, Accuracy: 81.63%\n",
      "Epoch [197/1000], Loss: 0.0303, Accuracy: 79.17%\n",
      "Epoch [198/1000], Loss: 0.1120, Accuracy: 79.17%\n",
      "Epoch [199/1000], Loss: 0.0683, Accuracy: 81.63%\n",
      "Epoch [200/1000], Loss: 0.0076, Accuracy: 80.85%\n",
      "Epoch [201/1000], Loss: 0.0241, Accuracy: 80.85%\n",
      "Epoch [202/1000], Loss: 0.3047, Accuracy: 80.00%\n",
      "Epoch [203/1000], Loss: 0.0856, Accuracy: 78.26%\n",
      "Epoch [204/1000], Loss: 0.2140, Accuracy: 81.63%\n",
      "Epoch [205/1000], Loss: 0.1184, Accuracy: 77.55%\n",
      "Epoch [206/1000], Loss: 0.0149, Accuracy: 79.07%\n",
      "Epoch [207/1000], Loss: 0.0278, Accuracy: 80.77%\n",
      "Epoch [208/1000], Loss: 0.0329, Accuracy: 80.85%\n",
      "Epoch [209/1000], Loss: 0.8603, Accuracy: 83.02%\n",
      "Epoch [210/1000], Loss: 0.2036, Accuracy: 81.63%\n",
      "Epoch [211/1000], Loss: 0.0119, Accuracy: 69.77%\n",
      "Epoch [212/1000], Loss: 0.1469, Accuracy: 72.34%\n",
      "Epoch [213/1000], Loss: 0.1150, Accuracy: 82.61%\n",
      "Epoch [214/1000], Loss: 0.0550, Accuracy: 83.02%\n",
      "Epoch [215/1000], Loss: 0.0890, Accuracy: 80.85%\n",
      "Epoch [216/1000], Loss: 0.0435, Accuracy: 81.63%\n",
      "Epoch [217/1000], Loss: 0.2347, Accuracy: 80.85%\n",
      "Epoch [218/1000], Loss: 0.2330, Accuracy: 80.00%\n",
      "Epoch [219/1000], Loss: 0.4272, Accuracy: 80.00%\n",
      "Epoch [220/1000], Loss: 0.1021, Accuracy: 77.27%\n",
      "Epoch [221/1000], Loss: 0.0438, Accuracy: 81.63%\n",
      "Epoch [222/1000], Loss: 0.4104, Accuracy: 71.43%\n",
      "Epoch [223/1000], Loss: 0.0997, Accuracy: 79.07%\n",
      "Epoch [224/1000], Loss: 0.1077, Accuracy: 78.43%\n",
      "Epoch [225/1000], Loss: 0.1224, Accuracy: 82.61%\n",
      "Epoch [226/1000], Loss: 0.0497, Accuracy: 80.00%\n",
      "Epoch [227/1000], Loss: 0.2924, Accuracy: 80.85%\n",
      "Epoch [228/1000], Loss: 0.0359, Accuracy: 79.17%\n",
      "Epoch [229/1000], Loss: 0.1869, Accuracy: 80.77%\n",
      "Epoch [230/1000], Loss: 0.1020, Accuracy: 80.00%\n",
      "Epoch [231/1000], Loss: 0.1859, Accuracy: 80.00%\n",
      "Epoch [232/1000], Loss: 0.0291, Accuracy: 82.35%\n",
      "Epoch [233/1000], Loss: 0.1760, Accuracy: 80.00%\n",
      "Epoch [234/1000], Loss: 0.0469, Accuracy: 81.63%\n",
      "Epoch [235/1000], Loss: 0.0198, Accuracy: 81.63%\n",
      "Epoch [236/1000], Loss: 0.0287, Accuracy: 79.17%\n",
      "Epoch [237/1000], Loss: 0.3329, Accuracy: 81.63%\n",
      "Epoch [238/1000], Loss: 0.2010, Accuracy: 84.00%\n",
      "Epoch [239/1000], Loss: 0.8382, Accuracy: 78.26%\n",
      "Epoch [240/1000], Loss: 0.0790, Accuracy: 78.26%\n",
      "Epoch [241/1000], Loss: 0.1599, Accuracy: 82.61%\n",
      "Epoch [242/1000], Loss: 0.7341, Accuracy: 84.00%\n",
      "Epoch [243/1000], Loss: 0.1076, Accuracy: 80.77%\n",
      "Epoch [244/1000], Loss: 0.0468, Accuracy: 80.00%\n",
      "Epoch [245/1000], Loss: 0.0983, Accuracy: 81.48%\n",
      "Epoch [246/1000], Loss: 0.0717, Accuracy: 79.17%\n",
      "Epoch [247/1000], Loss: 0.0796, Accuracy: 76.60%\n",
      "Epoch [248/1000], Loss: 0.6271, Accuracy: 79.25%\n",
      "Epoch [249/1000], Loss: 0.1220, Accuracy: 81.63%\n",
      "Epoch [250/1000], Loss: 0.0051, Accuracy: 85.11%\n",
      "Epoch [251/1000], Loss: 0.2003, Accuracy: 81.63%\n",
      "Epoch [252/1000], Loss: 0.0142, Accuracy: 78.26%\n",
      "Epoch [253/1000], Loss: 0.5429, Accuracy: 80.00%\n",
      "Epoch [254/1000], Loss: 0.2739, Accuracy: 80.85%\n",
      "Epoch [255/1000], Loss: 0.0429, Accuracy: 80.00%\n",
      "Epoch [256/1000], Loss: 0.4170, Accuracy: 77.55%\n",
      "Epoch [257/1000], Loss: 0.0894, Accuracy: 81.63%\n",
      "Epoch [258/1000], Loss: 0.6339, Accuracy: 79.17%\n",
      "Epoch [259/1000], Loss: 0.2332, Accuracy: 72.34%\n",
      "Epoch [260/1000], Loss: 0.0136, Accuracy: 77.27%\n",
      "Epoch [261/1000], Loss: 0.5327, Accuracy: 82.35%\n",
      "Epoch [262/1000], Loss: 0.1455, Accuracy: 81.82%\n",
      "Epoch [263/1000], Loss: 0.1210, Accuracy: 84.00%\n",
      "Epoch [264/1000], Loss: 0.0259, Accuracy: 73.91%\n",
      "Epoch [265/1000], Loss: 0.0332, Accuracy: 79.17%\n",
      "Epoch [266/1000], Loss: 0.1494, Accuracy: 76.60%\n",
      "Epoch [267/1000], Loss: 0.4328, Accuracy: 81.63%\n",
      "Epoch [268/1000], Loss: 0.0407, Accuracy: 79.17%\n",
      "Epoch [269/1000], Loss: 0.2542, Accuracy: 75.56%\n",
      "Epoch [270/1000], Loss: 0.0834, Accuracy: 73.91%\n",
      "Epoch [271/1000], Loss: 0.0876, Accuracy: 76.60%\n",
      "Epoch [272/1000], Loss: 0.0824, Accuracy: 84.00%\n",
      "Epoch [273/1000], Loss: 0.3956, Accuracy: 81.63%\n",
      "Epoch [274/1000], Loss: 0.4724, Accuracy: 76.92%\n",
      "Epoch [275/1000], Loss: 0.8523, Accuracy: 84.00%\n",
      "Epoch [276/1000], Loss: 0.2900, Accuracy: 82.61%\n",
      "Epoch [277/1000], Loss: 0.0839, Accuracy: 76.60%\n",
      "Epoch [278/1000], Loss: 0.0844, Accuracy: 76.60%\n",
      "Epoch [279/1000], Loss: 0.6244, Accuracy: 80.00%\n",
      "Epoch [280/1000], Loss: 0.3340, Accuracy: 84.00%\n",
      "Epoch [281/1000], Loss: 0.2662, Accuracy: 76.60%\n",
      "Epoch [282/1000], Loss: 0.0176, Accuracy: 75.00%\n",
      "Epoch [283/1000], Loss: 0.0948, Accuracy: 79.17%\n",
      "Epoch [284/1000], Loss: 0.0391, Accuracy: 80.77%\n",
      "Epoch [285/1000], Loss: 1.3352, Accuracy: 79.17%\n",
      "Epoch [286/1000], Loss: 0.2462, Accuracy: 76.60%\n",
      "Epoch [287/1000], Loss: 0.0625, Accuracy: 80.85%\n",
      "Epoch [288/1000], Loss: 0.1856, Accuracy: 81.63%\n",
      "Epoch [289/1000], Loss: 0.0709, Accuracy: 79.17%\n",
      "Epoch [290/1000], Loss: 0.0565, Accuracy: 79.17%\n",
      "Epoch [291/1000], Loss: 0.0248, Accuracy: 79.17%\n",
      "Epoch [292/1000], Loss: 0.5335, Accuracy: 78.26%\n",
      "Epoch [293/1000], Loss: 0.1199, Accuracy: 72.73%\n",
      "Epoch [294/1000], Loss: 0.2421, Accuracy: 77.55%\n",
      "Epoch [295/1000], Loss: 0.0693, Accuracy: 77.27%\n",
      "Epoch [296/1000], Loss: 0.0318, Accuracy: 83.02%\n",
      "Epoch [297/1000], Loss: 0.0431, Accuracy: 80.85%\n",
      "Epoch [298/1000], Loss: 0.2006, Accuracy: 76.92%\n",
      "Epoch [299/1000], Loss: 0.1650, Accuracy: 79.17%\n",
      "Epoch [300/1000], Loss: 0.1426, Accuracy: 73.91%\n",
      "Epoch [301/1000], Loss: 0.2150, Accuracy: 76.60%\n",
      "Epoch [302/1000], Loss: 0.0947, Accuracy: 80.00%\n",
      "Epoch [303/1000], Loss: 0.0151, Accuracy: 81.63%\n",
      "Epoch [304/1000], Loss: 0.0283, Accuracy: 81.63%\n",
      "Epoch [305/1000], Loss: 0.1661, Accuracy: 81.63%\n",
      "Epoch [306/1000], Loss: 0.3210, Accuracy: 86.27%\n",
      "Epoch [307/1000], Loss: 0.7429, Accuracy: 78.26%\n",
      "Epoch [308/1000], Loss: 0.1344, Accuracy: 72.73%\n",
      "Epoch [309/1000], Loss: 0.1486, Accuracy: 78.43%\n",
      "Epoch [310/1000], Loss: 0.1035, Accuracy: 83.33%\n",
      "Epoch [311/1000], Loss: 0.0567, Accuracy: 79.17%\n",
      "Epoch [312/1000], Loss: 0.1479, Accuracy: 78.43%\n",
      "Epoch [313/1000], Loss: 0.1693, Accuracy: 79.17%\n",
      "Epoch [314/1000], Loss: 0.4968, Accuracy: 80.85%\n",
      "Epoch [315/1000], Loss: 0.7949, Accuracy: 79.17%\n",
      "Epoch [316/1000], Loss: 0.0968, Accuracy: 73.47%\n",
      "Epoch [317/1000], Loss: 0.0728, Accuracy: 68.29%\n",
      "Epoch [318/1000], Loss: 0.1495, Accuracy: 76.92%\n",
      "Epoch [319/1000], Loss: 0.1074, Accuracy: 76.60%\n",
      "Epoch [320/1000], Loss: 0.0389, Accuracy: 79.17%\n",
      "Epoch [321/1000], Loss: 0.1300, Accuracy: 79.17%\n",
      "Epoch [322/1000], Loss: 0.6579, Accuracy: 77.55%\n",
      "Epoch [323/1000], Loss: 0.1638, Accuracy: 77.55%\n",
      "Epoch [324/1000], Loss: 0.0570, Accuracy: 75.56%\n",
      "Epoch [325/1000], Loss: 0.0473, Accuracy: 76.00%\n",
      "Epoch [326/1000], Loss: 0.1768, Accuracy: 78.26%\n",
      "Epoch [327/1000], Loss: 0.4512, Accuracy: 81.63%\n",
      "Epoch [328/1000], Loss: 0.0952, Accuracy: 86.27%\n",
      "Epoch [329/1000], Loss: 0.0706, Accuracy: 76.60%\n",
      "Epoch [330/1000], Loss: 0.0120, Accuracy: 76.60%\n",
      "Epoch [331/1000], Loss: 0.3402, Accuracy: 79.17%\n",
      "Epoch [332/1000], Loss: 0.2786, Accuracy: 78.43%\n",
      "Epoch [333/1000], Loss: 0.5989, Accuracy: 80.00%\n",
      "Epoch [334/1000], Loss: 0.5405, Accuracy: 84.62%\n",
      "Epoch [335/1000], Loss: 0.0949, Accuracy: 83.33%\n",
      "Epoch [336/1000], Loss: 0.2804, Accuracy: 73.91%\n",
      "Epoch [337/1000], Loss: 0.0491, Accuracy: 73.91%\n",
      "Epoch [338/1000], Loss: 0.0350, Accuracy: 76.60%\n",
      "Epoch [339/1000], Loss: 0.1886, Accuracy: 81.63%\n",
      "Epoch [340/1000], Loss: 0.0035, Accuracy: 81.63%\n",
      "Epoch [341/1000], Loss: 0.3914, Accuracy: 76.92%\n",
      "Epoch [342/1000], Loss: 0.0249, Accuracy: 78.26%\n",
      "Epoch [343/1000], Loss: 0.0549, Accuracy: 79.17%\n",
      "Epoch [344/1000], Loss: 0.0238, Accuracy: 80.00%\n",
      "Epoch [345/1000], Loss: 0.3708, Accuracy: 79.17%\n",
      "Epoch [346/1000], Loss: 0.5896, Accuracy: 79.25%\n",
      "Epoch [347/1000], Loss: 0.3340, Accuracy: 78.26%\n",
      "Epoch [348/1000], Loss: 0.0498, Accuracy: 71.43%\n",
      "Epoch [349/1000], Loss: 0.0841, Accuracy: 80.77%\n",
      "Epoch [350/1000], Loss: 0.1638, Accuracy: 78.26%\n",
      "Epoch [351/1000], Loss: 0.0670, Accuracy: 81.63%\n",
      "Epoch [352/1000], Loss: 0.0424, Accuracy: 81.63%\n",
      "Epoch [353/1000], Loss: 0.0092, Accuracy: 81.63%\n",
      "Epoch [354/1000], Loss: 0.0634, Accuracy: 81.63%\n",
      "Epoch [355/1000], Loss: 0.1373, Accuracy: 81.63%\n",
      "Epoch [356/1000], Loss: 0.0956, Accuracy: 81.63%\n",
      "Epoch [357/1000], Loss: 0.3624, Accuracy: 81.63%\n",
      "Epoch [358/1000], Loss: 0.0891, Accuracy: 81.63%\n",
      "Epoch [359/1000], Loss: 0.0280, Accuracy: 79.17%\n",
      "Epoch [360/1000], Loss: 0.0803, Accuracy: 81.63%\n",
      "Epoch [361/1000], Loss: 0.9139, Accuracy: 81.63%\n",
      "Epoch [362/1000], Loss: 0.0361, Accuracy: 81.63%\n",
      "Epoch [363/1000], Loss: 0.2349, Accuracy: 81.63%\n",
      "Epoch [364/1000], Loss: 0.2228, Accuracy: 73.47%\n",
      "Epoch [365/1000], Loss: 0.0710, Accuracy: 76.60%\n",
      "Epoch [366/1000], Loss: 0.3551, Accuracy: 80.00%\n",
      "Epoch [367/1000], Loss: 0.6352, Accuracy: 76.60%\n",
      "Epoch [368/1000], Loss: 0.0184, Accuracy: 71.79%\n",
      "Epoch [369/1000], Loss: 0.1173, Accuracy: 80.00%\n",
      "Epoch [370/1000], Loss: 0.2127, Accuracy: 80.85%\n",
      "Epoch [371/1000], Loss: 0.0368, Accuracy: 80.00%\n",
      "Epoch [372/1000], Loss: 0.0403, Accuracy: 79.17%\n",
      "Epoch [373/1000], Loss: 0.1569, Accuracy: 79.17%\n",
      "Epoch [374/1000], Loss: 0.1156, Accuracy: 76.60%\n",
      "Epoch [375/1000], Loss: 0.0653, Accuracy: 79.17%\n",
      "Epoch [376/1000], Loss: 0.0942, Accuracy: 76.60%\n",
      "Epoch [377/1000], Loss: 0.2687, Accuracy: 86.27%\n",
      "Epoch [378/1000], Loss: 0.0346, Accuracy: 84.00%\n",
      "Epoch [379/1000], Loss: 0.2507, Accuracy: 80.00%\n",
      "Epoch [380/1000], Loss: 0.6849, Accuracy: 80.00%\n",
      "Epoch [381/1000], Loss: 0.1444, Accuracy: 81.63%\n",
      "Epoch [382/1000], Loss: 0.5769, Accuracy: 81.48%\n",
      "Epoch [383/1000], Loss: 0.0952, Accuracy: 76.60%\n",
      "Epoch [384/1000], Loss: 0.1475, Accuracy: 78.26%\n",
      "Epoch [385/1000], Loss: 0.4121, Accuracy: 75.47%\n",
      "Epoch [386/1000], Loss: 0.1420, Accuracy: 81.63%\n",
      "Epoch [387/1000], Loss: 0.0443, Accuracy: 79.17%\n",
      "Epoch [388/1000], Loss: 0.1788, Accuracy: 78.43%\n",
      "Epoch [389/1000], Loss: 0.2136, Accuracy: 81.63%\n",
      "Epoch [390/1000], Loss: 0.0630, Accuracy: 79.17%\n",
      "Epoch [391/1000], Loss: 0.0627, Accuracy: 76.60%\n",
      "Epoch [392/1000], Loss: 0.0913, Accuracy: 76.60%\n",
      "Epoch [393/1000], Loss: 0.1536, Accuracy: 79.17%\n",
      "Epoch [394/1000], Loss: 0.0697, Accuracy: 76.60%\n",
      "Epoch [395/1000], Loss: 0.4707, Accuracy: 76.92%\n",
      "Epoch [396/1000], Loss: 0.0755, Accuracy: 69.77%\n",
      "Epoch [397/1000], Loss: 0.0152, Accuracy: 69.77%\n",
      "Epoch [398/1000], Loss: 0.2472, Accuracy: 69.39%\n",
      "Epoch [399/1000], Loss: 0.3785, Accuracy: 72.73%\n",
      "Epoch [400/1000], Loss: 0.0527, Accuracy: 80.00%\n",
      "Epoch [401/1000], Loss: 0.2168, Accuracy: 76.60%\n",
      "Epoch [402/1000], Loss: 0.0560, Accuracy: 75.00%\n",
      "Epoch [403/1000], Loss: 0.0869, Accuracy: 77.55%\n",
      "Epoch [404/1000], Loss: 0.0353, Accuracy: 73.91%\n",
      "Epoch [405/1000], Loss: 0.0620, Accuracy: 79.17%\n",
      "Epoch [406/1000], Loss: 0.1212, Accuracy: 79.17%\n",
      "Epoch [407/1000], Loss: 0.0652, Accuracy: 79.17%\n",
      "Epoch [408/1000], Loss: 0.0976, Accuracy: 79.17%\n",
      "Epoch [409/1000], Loss: 0.1941, Accuracy: 79.17%\n",
      "Epoch [410/1000], Loss: 0.3411, Accuracy: 84.00%\n",
      "Epoch [411/1000], Loss: 0.1639, Accuracy: 76.60%\n",
      "Epoch [412/1000], Loss: 0.0116, Accuracy: 68.18%\n",
      "Epoch [413/1000], Loss: 0.7189, Accuracy: 74.07%\n",
      "Epoch [414/1000], Loss: 0.4201, Accuracy: 71.43%\n",
      "Epoch [415/1000], Loss: 0.4696, Accuracy: 59.46%\n",
      "Epoch [416/1000], Loss: 0.1124, Accuracy: 71.19%\n",
      "Epoch [417/1000], Loss: 0.1289, Accuracy: 74.42%\n",
      "Epoch [418/1000], Loss: 0.0168, Accuracy: 80.95%\n",
      "Epoch [419/1000], Loss: 0.6191, Accuracy: 73.47%\n",
      "Epoch [420/1000], Loss: 0.4706, Accuracy: 72.73%\n",
      "Epoch [421/1000], Loss: 0.0460, Accuracy: 81.63%\n",
      "Epoch [422/1000], Loss: 0.1671, Accuracy: 81.63%\n",
      "Epoch [423/1000], Loss: 0.1294, Accuracy: 79.17%\n",
      "Epoch [424/1000], Loss: 0.0241, Accuracy: 76.60%\n",
      "Epoch [425/1000], Loss: 0.8806, Accuracy: 76.60%\n",
      "Epoch [426/1000], Loss: 0.1636, Accuracy: 81.63%\n",
      "Epoch [427/1000], Loss: 0.0679, Accuracy: 81.63%\n",
      "Epoch [428/1000], Loss: 0.2172, Accuracy: 76.60%\n",
      "Epoch [429/1000], Loss: 0.7741, Accuracy: 79.17%\n",
      "Epoch [430/1000], Loss: 0.0659, Accuracy: 76.92%\n",
      "Epoch [431/1000], Loss: 0.0137, Accuracy: 74.42%\n",
      "Epoch [432/1000], Loss: 0.2286, Accuracy: 79.17%\n",
      "Epoch [433/1000], Loss: 0.0538, Accuracy: 81.63%\n",
      "Epoch [434/1000], Loss: 0.3265, Accuracy: 79.17%\n",
      "Epoch [435/1000], Loss: 0.4015, Accuracy: 79.17%\n",
      "Epoch [436/1000], Loss: 0.0121, Accuracy: 80.00%\n",
      "Epoch [437/1000], Loss: 0.2932, Accuracy: 75.47%\n",
      "Epoch [438/1000], Loss: 0.6704, Accuracy: 79.17%\n",
      "Epoch [439/1000], Loss: 0.3466, Accuracy: 81.63%\n",
      "Epoch [440/1000], Loss: 0.0834, Accuracy: 81.63%\n",
      "Epoch [441/1000], Loss: 0.0925, Accuracy: 78.26%\n",
      "Epoch [442/1000], Loss: 0.0607, Accuracy: 81.63%\n",
      "Epoch [443/1000], Loss: 0.0304, Accuracy: 79.17%\n",
      "Epoch [444/1000], Loss: 0.1071, Accuracy: 79.17%\n",
      "Epoch [445/1000], Loss: 0.5553, Accuracy: 72.73%\n",
      "Epoch [446/1000], Loss: 0.8759, Accuracy: 75.56%\n",
      "Epoch [447/1000], Loss: 0.8221, Accuracy: 80.70%\n",
      "Epoch [448/1000], Loss: 0.0098, Accuracy: 80.85%\n",
      "Epoch [449/1000], Loss: 0.0090, Accuracy: 77.55%\n",
      "Epoch [450/1000], Loss: 0.1481, Accuracy: 77.55%\n",
      "Epoch [451/1000], Loss: 0.0254, Accuracy: 75.00%\n",
      "Epoch [452/1000], Loss: 0.1038, Accuracy: 76.60%\n",
      "Epoch [453/1000], Loss: 0.9251, Accuracy: 75.00%\n",
      "Epoch [454/1000], Loss: 0.1937, Accuracy: 73.47%\n",
      "Epoch [455/1000], Loss: 0.0566, Accuracy: 73.91%\n",
      "Epoch [456/1000], Loss: 0.2189, Accuracy: 73.91%\n",
      "Epoch [457/1000], Loss: 0.0434, Accuracy: 73.91%\n",
      "Epoch [458/1000], Loss: 0.1206, Accuracy: 73.47%\n",
      "Epoch [459/1000], Loss: 0.8216, Accuracy: 76.00%\n",
      "Epoch [460/1000], Loss: 0.1908, Accuracy: 73.91%\n",
      "Epoch [461/1000], Loss: 0.0089, Accuracy: 71.43%\n",
      "Epoch [462/1000], Loss: 0.2517, Accuracy: 79.17%\n",
      "Epoch [463/1000], Loss: 0.0297, Accuracy: 73.91%\n",
      "Epoch [464/1000], Loss: 0.0825, Accuracy: 75.00%\n",
      "Epoch [465/1000], Loss: 0.4948, Accuracy: 76.92%\n",
      "Epoch [466/1000], Loss: 0.0077, Accuracy: 78.43%\n",
      "Epoch [467/1000], Loss: 0.4550, Accuracy: 73.91%\n",
      "Epoch [468/1000], Loss: 0.0721, Accuracy: 76.00%\n",
      "Epoch [469/1000], Loss: 0.5175, Accuracy: 73.91%\n",
      "Epoch [470/1000], Loss: 0.3543, Accuracy: 73.91%\n",
      "Epoch [471/1000], Loss: 0.6588, Accuracy: 72.00%\n",
      "Epoch [472/1000], Loss: 1.4538, Accuracy: 78.26%\n",
      "Epoch [473/1000], Loss: 0.2675, Accuracy: 76.19%\n",
      "Epoch [474/1000], Loss: 0.1009, Accuracy: 73.47%\n",
      "Epoch [475/1000], Loss: 0.1273, Accuracy: 72.73%\n",
      "Epoch [476/1000], Loss: 0.0669, Accuracy: 76.60%\n",
      "Epoch [477/1000], Loss: 0.8303, Accuracy: 73.91%\n",
      "Epoch [478/1000], Loss: 0.0176, Accuracy: 70.00%\n",
      "Epoch [479/1000], Loss: 0.0671, Accuracy: 70.59%\n",
      "Epoch [480/1000], Loss: 0.1833, Accuracy: 72.73%\n",
      "Epoch [481/1000], Loss: 0.0132, Accuracy: 76.60%\n",
      "Epoch [482/1000], Loss: 1.7334, Accuracy: 76.00%\n",
      "Epoch [483/1000], Loss: 0.3938, Accuracy: 79.17%\n",
      "Epoch [484/1000], Loss: 0.0698, Accuracy: 73.91%\n",
      "Epoch [485/1000], Loss: 0.1297, Accuracy: 72.73%\n",
      "Epoch [486/1000], Loss: 0.0997, Accuracy: 73.47%\n",
      "Epoch [487/1000], Loss: 0.1214, Accuracy: 73.91%\n",
      "Epoch [488/1000], Loss: 0.1129, Accuracy: 75.00%\n",
      "Epoch [489/1000], Loss: 0.1505, Accuracy: 76.60%\n",
      "Epoch [490/1000], Loss: 0.3641, Accuracy: 73.91%\n",
      "Epoch [491/1000], Loss: 0.0092, Accuracy: 73.17%\n",
      "Epoch [492/1000], Loss: 0.1561, Accuracy: 76.92%\n",
      "Epoch [493/1000], Loss: 0.0110, Accuracy: 75.56%\n",
      "Epoch [494/1000], Loss: 0.1235, Accuracy: 76.60%\n",
      "Epoch [495/1000], Loss: 0.0910, Accuracy: 79.17%\n",
      "Epoch [496/1000], Loss: 0.1623, Accuracy: 79.17%\n",
      "Epoch [497/1000], Loss: 0.1046, Accuracy: 73.91%\n",
      "Epoch [498/1000], Loss: 0.0598, Accuracy: 78.26%\n",
      "Epoch [499/1000], Loss: 0.2789, Accuracy: 78.26%\n",
      "Epoch [500/1000], Loss: 0.9339, Accuracy: 79.17%\n",
      "Epoch [501/1000], Loss: 0.2183, Accuracy: 80.85%\n",
      "Epoch [502/1000], Loss: 0.1582, Accuracy: 77.55%\n",
      "Epoch [503/1000], Loss: 0.2994, Accuracy: 76.60%\n",
      "Epoch [504/1000], Loss: 0.0632, Accuracy: 75.56%\n",
      "Epoch [505/1000], Loss: 0.2698, Accuracy: 76.60%\n",
      "Epoch [506/1000], Loss: 0.2408, Accuracy: 79.17%\n",
      "Epoch [507/1000], Loss: 0.1475, Accuracy: 79.17%\n",
      "Epoch [508/1000], Loss: 0.0893, Accuracy: 76.60%\n",
      "Epoch [509/1000], Loss: 0.0262, Accuracy: 73.91%\n",
      "Epoch [510/1000], Loss: 0.6248, Accuracy: 76.00%\n",
      "Epoch [511/1000], Loss: 0.3794, Accuracy: 76.00%\n",
      "Epoch [512/1000], Loss: 0.3755, Accuracy: 76.60%\n",
      "Epoch [513/1000], Loss: 0.6451, Accuracy: 73.91%\n",
      "Epoch [514/1000], Loss: 0.5131, Accuracy: 78.43%\n",
      "Epoch [515/1000], Loss: 0.6894, Accuracy: 76.60%\n",
      "Epoch [516/1000], Loss: 0.0401, Accuracy: 74.42%\n",
      "Epoch [517/1000], Loss: 0.0680, Accuracy: 79.17%\n",
      "Epoch [518/1000], Loss: 0.1148, Accuracy: 76.60%\n",
      "Epoch [519/1000], Loss: 0.1002, Accuracy: 72.34%\n",
      "Epoch [520/1000], Loss: 0.1655, Accuracy: 75.56%\n",
      "Epoch [521/1000], Loss: 0.1492, Accuracy: 72.73%\n",
      "Epoch [522/1000], Loss: 0.2805, Accuracy: 74.51%\n",
      "Epoch [523/1000], Loss: 0.0353, Accuracy: 73.91%\n",
      "Epoch [524/1000], Loss: 0.0413, Accuracy: 73.91%\n",
      "Epoch [525/1000], Loss: 0.1839, Accuracy: 74.51%\n",
      "Epoch [526/1000], Loss: 0.2066, Accuracy: 76.60%\n",
      "Epoch [527/1000], Loss: 0.0032, Accuracy: 79.17%\n",
      "Epoch [528/1000], Loss: 0.0029, Accuracy: 79.17%\n",
      "Epoch [529/1000], Loss: 0.0560, Accuracy: 79.17%\n",
      "Epoch [530/1000], Loss: 0.2838, Accuracy: 79.17%\n",
      "Epoch [531/1000], Loss: 0.2803, Accuracy: 73.47%\n",
      "Epoch [532/1000], Loss: 0.2618, Accuracy: 70.83%\n",
      "Epoch [533/1000], Loss: 0.2165, Accuracy: 72.73%\n",
      "Epoch [534/1000], Loss: 0.1530, Accuracy: 76.00%\n",
      "Epoch [535/1000], Loss: 0.0442, Accuracy: 73.91%\n",
      "Epoch [536/1000], Loss: 0.0993, Accuracy: 79.17%\n",
      "Epoch [537/1000], Loss: 0.2121, Accuracy: 73.91%\n",
      "Epoch [538/1000], Loss: 0.0960, Accuracy: 79.17%\n",
      "Epoch [539/1000], Loss: 0.6813, Accuracy: 79.25%\n",
      "Epoch [540/1000], Loss: 0.3790, Accuracy: 80.00%\n",
      "Epoch [541/1000], Loss: 0.1123, Accuracy: 76.60%\n",
      "Epoch [542/1000], Loss: 0.0407, Accuracy: 71.11%\n",
      "Epoch [543/1000], Loss: 0.1233, Accuracy: 73.91%\n",
      "Epoch [544/1000], Loss: 2.1884, Accuracy: 76.00%\n",
      "Epoch [545/1000], Loss: 0.1145, Accuracy: 81.63%\n",
      "Epoch [546/1000], Loss: 0.0365, Accuracy: 73.17%\n",
      "Epoch [547/1000], Loss: 1.1153, Accuracy: 75.56%\n",
      "Epoch [548/1000], Loss: 0.2469, Accuracy: 76.19%\n",
      "Epoch [549/1000], Loss: 0.4125, Accuracy: 76.00%\n",
      "Epoch [550/1000], Loss: 0.0372, Accuracy: 73.91%\n",
      "Epoch [551/1000], Loss: 0.0350, Accuracy: 73.47%\n",
      "Epoch [552/1000], Loss: 0.8306, Accuracy: 75.56%\n",
      "Epoch [553/1000], Loss: 0.4485, Accuracy: 77.55%\n",
      "Epoch [554/1000], Loss: 0.0482, Accuracy: 78.43%\n",
      "Epoch [555/1000], Loss: 0.1007, Accuracy: 77.27%\n",
      "Epoch [556/1000], Loss: 0.0416, Accuracy: 74.51%\n",
      "Epoch [557/1000], Loss: 0.6843, Accuracy: 79.17%\n",
      "Epoch [558/1000], Loss: 0.1390, Accuracy: 75.00%\n",
      "Epoch [559/1000], Loss: 0.0514, Accuracy: 72.73%\n",
      "Epoch [560/1000], Loss: 0.1227, Accuracy: 73.47%\n",
      "Epoch [561/1000], Loss: 0.2222, Accuracy: 75.00%\n",
      "Epoch [562/1000], Loss: 0.1046, Accuracy: 77.55%\n",
      "Epoch [563/1000], Loss: 0.0179, Accuracy: 73.91%\n",
      "Epoch [564/1000], Loss: 0.4180, Accuracy: 73.91%\n",
      "Epoch [565/1000], Loss: 0.0281, Accuracy: 76.60%\n",
      "Epoch [566/1000], Loss: 0.0340, Accuracy: 77.55%\n",
      "Epoch [567/1000], Loss: 0.0018, Accuracy: 79.17%\n",
      "Epoch [568/1000], Loss: 0.0349, Accuracy: 77.55%\n",
      "Epoch [569/1000], Loss: 0.0822, Accuracy: 76.60%\n",
      "Epoch [570/1000], Loss: 0.1673, Accuracy: 76.60%\n",
      "Epoch [571/1000], Loss: 0.6623, Accuracy: 78.26%\n",
      "Epoch [572/1000], Loss: 0.0249, Accuracy: 76.00%\n",
      "Epoch [573/1000], Loss: 0.0466, Accuracy: 78.26%\n",
      "Epoch [574/1000], Loss: 0.0613, Accuracy: 77.55%\n",
      "Epoch [575/1000], Loss: 0.1960, Accuracy: 76.60%\n",
      "Epoch [576/1000], Loss: 0.1443, Accuracy: 76.60%\n",
      "Epoch [577/1000], Loss: 0.0915, Accuracy: 72.73%\n",
      "Epoch [578/1000], Loss: 0.1021, Accuracy: 76.60%\n",
      "Epoch [579/1000], Loss: 0.0558, Accuracy: 79.17%\n",
      "Epoch [580/1000], Loss: 0.0781, Accuracy: 79.17%\n",
      "Epoch [581/1000], Loss: 0.0489, Accuracy: 79.17%\n",
      "Epoch [582/1000], Loss: 0.1261, Accuracy: 79.17%\n",
      "Epoch [583/1000], Loss: 0.0540, Accuracy: 76.60%\n",
      "Epoch [584/1000], Loss: 0.1799, Accuracy: 79.17%\n",
      "Epoch [585/1000], Loss: 0.0878, Accuracy: 79.17%\n",
      "Epoch [586/1000], Loss: 0.5834, Accuracy: 77.55%\n",
      "Epoch [587/1000], Loss: 0.0884, Accuracy: 82.35%\n",
      "Epoch [588/1000], Loss: 0.0191, Accuracy: 76.60%\n",
      "Epoch [589/1000], Loss: 0.2261, Accuracy: 74.51%\n",
      "Epoch [590/1000], Loss: 0.0209, Accuracy: 79.17%\n",
      "Epoch [591/1000], Loss: 0.2086, Accuracy: 76.60%\n",
      "Epoch [592/1000], Loss: 0.0497, Accuracy: 84.00%\n",
      "Epoch [593/1000], Loss: 0.0031, Accuracy: 81.63%\n",
      "Epoch [594/1000], Loss: 0.0370, Accuracy: 77.55%\n",
      "Epoch [595/1000], Loss: 0.1788, Accuracy: 77.55%\n",
      "Epoch [596/1000], Loss: 0.0114, Accuracy: 78.26%\n",
      "Epoch [597/1000], Loss: 0.0122, Accuracy: 79.17%\n",
      "Epoch [598/1000], Loss: 0.0571, Accuracy: 81.63%\n",
      "Epoch [599/1000], Loss: 1.1872, Accuracy: 81.63%\n",
      "Epoch [600/1000], Loss: 0.0930, Accuracy: 76.00%\n",
      "Epoch [601/1000], Loss: 0.0778, Accuracy: 75.56%\n",
      "Epoch [602/1000], Loss: 0.0791, Accuracy: 70.83%\n",
      "Epoch [603/1000], Loss: 0.0235, Accuracy: 76.60%\n",
      "Epoch [604/1000], Loss: 0.2265, Accuracy: 78.43%\n",
      "Epoch [605/1000], Loss: 0.0106, Accuracy: 73.47%\n",
      "Epoch [606/1000], Loss: 0.0275, Accuracy: 72.73%\n",
      "Epoch [607/1000], Loss: 0.6839, Accuracy: 73.08%\n",
      "Epoch [608/1000], Loss: 0.2469, Accuracy: 75.56%\n",
      "Epoch [609/1000], Loss: 0.0106, Accuracy: 72.73%\n",
      "Epoch [610/1000], Loss: 0.4849, Accuracy: 77.55%\n",
      "Epoch [611/1000], Loss: 0.1300, Accuracy: 80.85%\n",
      "Epoch [612/1000], Loss: 0.3096, Accuracy: 78.43%\n",
      "Epoch [613/1000], Loss: 0.0252, Accuracy: 75.56%\n",
      "Epoch [614/1000], Loss: 0.0869, Accuracy: 79.17%\n",
      "Epoch [615/1000], Loss: 0.0314, Accuracy: 73.47%\n",
      "Epoch [616/1000], Loss: 0.0766, Accuracy: 76.60%\n",
      "Epoch [617/1000], Loss: 0.0065, Accuracy: 76.60%\n",
      "Epoch [618/1000], Loss: 0.1260, Accuracy: 76.00%\n",
      "Epoch [619/1000], Loss: 0.2572, Accuracy: 73.91%\n",
      "Epoch [620/1000], Loss: 0.0793, Accuracy: 78.26%\n",
      "Epoch [621/1000], Loss: 0.1804, Accuracy: 75.47%\n",
      "Epoch [622/1000], Loss: 0.1263, Accuracy: 76.60%\n",
      "Epoch [623/1000], Loss: 0.0938, Accuracy: 81.63%\n",
      "Epoch [624/1000], Loss: 0.0401, Accuracy: 81.63%\n",
      "Epoch [625/1000], Loss: 0.1720, Accuracy: 79.17%\n",
      "Epoch [626/1000], Loss: 0.2437, Accuracy: 79.17%\n",
      "Epoch [627/1000], Loss: 0.0441, Accuracy: 78.43%\n",
      "Epoch [628/1000], Loss: 0.0578, Accuracy: 79.17%\n",
      "Epoch [629/1000], Loss: 0.4248, Accuracy: 75.56%\n",
      "Epoch [630/1000], Loss: 0.1340, Accuracy: 75.56%\n",
      "Epoch [631/1000], Loss: 0.0254, Accuracy: 77.55%\n",
      "Epoch [632/1000], Loss: 0.1186, Accuracy: 78.43%\n",
      "Epoch [633/1000], Loss: 0.0093, Accuracy: 77.55%\n",
      "Epoch [634/1000], Loss: 0.1151, Accuracy: 79.17%\n",
      "Epoch [635/1000], Loss: 0.5647, Accuracy: 76.92%\n",
      "Epoch [636/1000], Loss: 0.0944, Accuracy: 78.43%\n",
      "Epoch [637/1000], Loss: 0.0611, Accuracy: 77.55%\n",
      "Epoch [638/1000], Loss: 0.0617, Accuracy: 76.00%\n",
      "Epoch [639/1000], Loss: 0.0920, Accuracy: 80.00%\n",
      "Epoch [640/1000], Loss: 0.0258, Accuracy: 76.60%\n",
      "Epoch [641/1000], Loss: 0.2403, Accuracy: 73.47%\n",
      "Epoch [642/1000], Loss: 0.0074, Accuracy: 76.00%\n",
      "Epoch [643/1000], Loss: 0.0085, Accuracy: 76.60%\n",
      "Epoch [644/1000], Loss: 0.0951, Accuracy: 76.00%\n",
      "Epoch [645/1000], Loss: 0.0178, Accuracy: 79.17%\n",
      "Epoch [646/1000], Loss: 0.1921, Accuracy: 79.17%\n",
      "Epoch [647/1000], Loss: 0.0395, Accuracy: 79.17%\n",
      "Epoch [648/1000], Loss: 0.0724, Accuracy: 76.00%\n",
      "Epoch [649/1000], Loss: 0.1462, Accuracy: 78.43%\n",
      "Epoch [650/1000], Loss: 1.5846, Accuracy: 80.77%\n",
      "Epoch [651/1000], Loss: 0.3629, Accuracy: 81.63%\n",
      "Epoch [652/1000], Loss: 0.0448, Accuracy: 75.56%\n",
      "Epoch [653/1000], Loss: 0.0467, Accuracy: 76.00%\n",
      "Epoch [654/1000], Loss: 0.2109, Accuracy: 76.60%\n",
      "Epoch [655/1000], Loss: 0.4337, Accuracy: 76.00%\n",
      "Epoch [656/1000], Loss: 0.2490, Accuracy: 77.55%\n",
      "Epoch [657/1000], Loss: 0.0147, Accuracy: 71.43%\n",
      "Epoch [658/1000], Loss: 0.6035, Accuracy: 73.08%\n",
      "Epoch [659/1000], Loss: 0.0298, Accuracy: 79.17%\n",
      "Epoch [660/1000], Loss: 0.0153, Accuracy: 76.60%\n",
      "Epoch [661/1000], Loss: 0.0057, Accuracy: 73.47%\n",
      "Epoch [662/1000], Loss: 0.7958, Accuracy: 72.00%\n",
      "Epoch [663/1000], Loss: 0.0284, Accuracy: 76.00%\n",
      "Epoch [664/1000], Loss: 0.0227, Accuracy: 75.56%\n",
      "Epoch [665/1000], Loss: 0.0324, Accuracy: 73.47%\n",
      "Epoch [666/1000], Loss: 0.0491, Accuracy: 78.26%\n",
      "Epoch [667/1000], Loss: 0.0607, Accuracy: 76.60%\n",
      "Epoch [668/1000], Loss: 0.1745, Accuracy: 76.60%\n",
      "Epoch [669/1000], Loss: 0.0832, Accuracy: 76.60%\n",
      "Epoch [670/1000], Loss: 0.2477, Accuracy: 74.51%\n",
      "Epoch [671/1000], Loss: 0.1371, Accuracy: 73.91%\n",
      "Epoch [672/1000], Loss: 0.0146, Accuracy: 78.26%\n",
      "Epoch [673/1000], Loss: 0.4551, Accuracy: 77.78%\n",
      "Epoch [674/1000], Loss: 0.1425, Accuracy: 75.56%\n",
      "Epoch [675/1000], Loss: 0.0306, Accuracy: 75.56%\n",
      "Epoch [676/1000], Loss: 0.6535, Accuracy: 76.00%\n",
      "Epoch [677/1000], Loss: 0.1183, Accuracy: 80.00%\n",
      "Epoch [678/1000], Loss: 0.0124, Accuracy: 79.25%\n",
      "Epoch [679/1000], Loss: 0.0823, Accuracy: 75.56%\n",
      "Epoch [680/1000], Loss: 0.1098, Accuracy: 72.34%\n",
      "Epoch [681/1000], Loss: 0.0953, Accuracy: 75.56%\n",
      "Epoch [682/1000], Loss: 0.0307, Accuracy: 78.26%\n",
      "Epoch [683/1000], Loss: 0.0381, Accuracy: 76.00%\n",
      "Epoch [684/1000], Loss: 0.0989, Accuracy: 76.60%\n",
      "Epoch [685/1000], Loss: 0.1325, Accuracy: 76.00%\n",
      "Epoch [686/1000], Loss: 0.1215, Accuracy: 73.47%\n",
      "Epoch [687/1000], Loss: 0.0150, Accuracy: 75.56%\n",
      "Epoch [688/1000], Loss: 0.0065, Accuracy: 75.00%\n",
      "Epoch [689/1000], Loss: 0.0846, Accuracy: 79.17%\n",
      "Epoch [690/1000], Loss: 0.0356, Accuracy: 76.60%\n",
      "Epoch [691/1000], Loss: 0.0301, Accuracy: 76.00%\n",
      "Epoch [692/1000], Loss: 1.0266, Accuracy: 72.73%\n",
      "Epoch [693/1000], Loss: 0.4242, Accuracy: 73.17%\n",
      "Epoch [694/1000], Loss: 0.0712, Accuracy: 72.00%\n",
      "Epoch [695/1000], Loss: 0.0272, Accuracy: 69.77%\n",
      "Epoch [696/1000], Loss: 0.0176, Accuracy: 80.77%\n",
      "Epoch [697/1000], Loss: 0.0529, Accuracy: 79.17%\n",
      "Epoch [698/1000], Loss: 0.0497, Accuracy: 73.91%\n",
      "Epoch [699/1000], Loss: 0.1069, Accuracy: 75.56%\n",
      "Epoch [700/1000], Loss: 0.0390, Accuracy: 73.47%\n",
      "Epoch [701/1000], Loss: 0.0652, Accuracy: 73.47%\n",
      "Epoch [702/1000], Loss: 0.1573, Accuracy: 76.00%\n",
      "Epoch [703/1000], Loss: 0.2519, Accuracy: 75.56%\n",
      "Epoch [704/1000], Loss: 0.5475, Accuracy: 75.00%\n",
      "Epoch [705/1000], Loss: 0.3826, Accuracy: 69.57%\n",
      "Epoch [706/1000], Loss: 1.7294, Accuracy: 65.00%\n",
      "Epoch [707/1000], Loss: 0.9530, Accuracy: 71.43%\n",
      "Epoch [708/1000], Loss: 0.0105, Accuracy: 70.27%\n",
      "Epoch [709/1000], Loss: 1.0416, Accuracy: 76.60%\n",
      "Epoch [710/1000], Loss: 0.0068, Accuracy: 78.57%\n",
      "Epoch [711/1000], Loss: 0.3028, Accuracy: 81.82%\n",
      "Epoch [712/1000], Loss: 0.0235, Accuracy: 78.05%\n",
      "Epoch [713/1000], Loss: 0.1366, Accuracy: 76.92%\n",
      "Epoch [714/1000], Loss: 0.6576, Accuracy: 76.00%\n",
      "Epoch [715/1000], Loss: 0.1104, Accuracy: 70.83%\n",
      "Epoch [716/1000], Loss: 0.2780, Accuracy: 75.56%\n",
      "Epoch [717/1000], Loss: 0.0411, Accuracy: 73.91%\n",
      "Epoch [718/1000], Loss: 0.1758, Accuracy: 75.56%\n",
      "Epoch [719/1000], Loss: 0.0730, Accuracy: 77.27%\n",
      "Epoch [720/1000], Loss: 0.1489, Accuracy: 78.43%\n",
      "Epoch [721/1000], Loss: 0.2006, Accuracy: 78.26%\n",
      "Epoch [722/1000], Loss: 0.0309, Accuracy: 79.17%\n",
      "Epoch [723/1000], Loss: 0.2258, Accuracy: 76.92%\n",
      "Epoch [724/1000], Loss: 0.6180, Accuracy: 76.60%\n",
      "Epoch [725/1000], Loss: 0.0079, Accuracy: 74.42%\n",
      "Epoch [726/1000], Loss: 0.1392, Accuracy: 77.55%\n",
      "Epoch [727/1000], Loss: 0.0545, Accuracy: 76.60%\n",
      "Epoch [728/1000], Loss: 1.7337, Accuracy: 76.60%\n",
      "Epoch [729/1000], Loss: 0.5665, Accuracy: 76.36%\n",
      "Epoch [730/1000], Loss: 0.8286, Accuracy: 71.11%\n",
      "Epoch [731/1000], Loss: 0.5285, Accuracy: 71.43%\n",
      "Epoch [732/1000], Loss: 0.0704, Accuracy: 78.43%\n",
      "Epoch [733/1000], Loss: 0.0791, Accuracy: 75.56%\n",
      "Epoch [734/1000], Loss: 0.3124, Accuracy: 79.17%\n",
      "Epoch [735/1000], Loss: 0.2171, Accuracy: 76.00%\n",
      "Epoch [736/1000], Loss: 0.1599, Accuracy: 74.42%\n",
      "Epoch [737/1000], Loss: 0.0426, Accuracy: 78.43%\n",
      "Epoch [738/1000], Loss: 0.1287, Accuracy: 77.55%\n",
      "Epoch [739/1000], Loss: 0.0072, Accuracy: 79.17%\n",
      "Epoch [740/1000], Loss: 0.0915, Accuracy: 80.85%\n",
      "Epoch [741/1000], Loss: 0.0076, Accuracy: 79.17%\n",
      "Epoch [742/1000], Loss: 0.1028, Accuracy: 79.17%\n",
      "Epoch [743/1000], Loss: 0.0081, Accuracy: 79.17%\n",
      "Epoch [744/1000], Loss: 0.8960, Accuracy: 79.17%\n",
      "Epoch [745/1000], Loss: 0.0445, Accuracy: 77.55%\n",
      "Epoch [746/1000], Loss: 0.3823, Accuracy: 73.91%\n",
      "Epoch [747/1000], Loss: 0.0941, Accuracy: 76.00%\n",
      "Epoch [748/1000], Loss: 0.1892, Accuracy: 77.55%\n",
      "Epoch [749/1000], Loss: 0.0107, Accuracy: 76.00%\n",
      "Epoch [750/1000], Loss: 0.2019, Accuracy: 70.83%\n",
      "Epoch [751/1000], Loss: 0.8695, Accuracy: 76.00%\n",
      "Epoch [752/1000], Loss: 0.1128, Accuracy: 73.47%\n",
      "Epoch [753/1000], Loss: 0.0200, Accuracy: 78.26%\n",
      "Epoch [754/1000], Loss: 0.3856, Accuracy: 72.34%\n",
      "Epoch [755/1000], Loss: 0.0597, Accuracy: 76.00%\n",
      "Epoch [756/1000], Loss: 0.5163, Accuracy: 79.17%\n",
      "Epoch [757/1000], Loss: 0.0477, Accuracy: 84.00%\n",
      "Epoch [758/1000], Loss: 0.1728, Accuracy: 80.85%\n",
      "Epoch [759/1000], Loss: 0.2207, Accuracy: 78.26%\n",
      "Epoch [760/1000], Loss: 0.0109, Accuracy: 80.00%\n",
      "Epoch [761/1000], Loss: 0.0723, Accuracy: 79.17%\n",
      "Epoch [762/1000], Loss: 0.1458, Accuracy: 79.17%\n",
      "Epoch [763/1000], Loss: 0.0099, Accuracy: 75.56%\n",
      "Epoch [764/1000], Loss: 0.4483, Accuracy: 76.00%\n",
      "Epoch [765/1000], Loss: 0.0496, Accuracy: 76.00%\n",
      "Epoch [766/1000], Loss: 0.0077, Accuracy: 80.00%\n",
      "Epoch [767/1000], Loss: 0.0263, Accuracy: 76.92%\n",
      "Epoch [768/1000], Loss: 0.0584, Accuracy: 79.17%\n",
      "Epoch [769/1000], Loss: 0.0356, Accuracy: 79.17%\n",
      "Epoch [770/1000], Loss: 0.1526, Accuracy: 79.17%\n",
      "Epoch [771/1000], Loss: 0.0329, Accuracy: 76.60%\n",
      "Epoch [772/1000], Loss: 0.0181, Accuracy: 76.60%\n",
      "Epoch [773/1000], Loss: 0.0811, Accuracy: 76.60%\n",
      "Epoch [774/1000], Loss: 0.3738, Accuracy: 78.26%\n",
      "Epoch [775/1000], Loss: 0.0225, Accuracy: 78.26%\n",
      "Epoch [776/1000], Loss: 1.4232, Accuracy: 76.92%\n",
      "Epoch [777/1000], Loss: 0.2040, Accuracy: 78.26%\n",
      "Epoch [778/1000], Loss: 0.0343, Accuracy: 78.26%\n",
      "Epoch [779/1000], Loss: 0.3618, Accuracy: 75.56%\n",
      "Epoch [780/1000], Loss: 0.0580, Accuracy: 68.29%\n",
      "Epoch [781/1000], Loss: 0.0490, Accuracy: 76.92%\n",
      "Epoch [782/1000], Loss: 0.2245, Accuracy: 75.00%\n",
      "Epoch [783/1000], Loss: 0.3250, Accuracy: 74.51%\n",
      "Epoch [784/1000], Loss: 0.4702, Accuracy: 80.00%\n",
      "Epoch [785/1000], Loss: 0.1814, Accuracy: 75.00%\n",
      "Epoch [786/1000], Loss: 0.0683, Accuracy: 76.60%\n",
      "Epoch [787/1000], Loss: 0.0154, Accuracy: 76.00%\n",
      "Epoch [788/1000], Loss: 0.1216, Accuracy: 77.55%\n",
      "Epoch [789/1000], Loss: 0.0181, Accuracy: 78.43%\n",
      "Epoch [790/1000], Loss: 0.0647, Accuracy: 77.55%\n",
      "Epoch [791/1000], Loss: 0.0131, Accuracy: 77.55%\n",
      "Epoch [792/1000], Loss: 0.0136, Accuracy: 79.17%\n",
      "Epoch [793/1000], Loss: 0.0241, Accuracy: 77.55%\n",
      "Epoch [794/1000], Loss: 0.0407, Accuracy: 77.55%\n",
      "Epoch [795/1000], Loss: 0.0221, Accuracy: 79.17%\n",
      "Epoch [796/1000], Loss: 0.2299, Accuracy: 76.60%\n",
      "Epoch [797/1000], Loss: 0.6716, Accuracy: 78.43%\n",
      "Epoch [798/1000], Loss: 0.0936, Accuracy: 80.77%\n",
      "Epoch [799/1000], Loss: 0.4175, Accuracy: 76.00%\n",
      "Epoch [800/1000], Loss: 0.0874, Accuracy: 74.51%\n",
      "Epoch [801/1000], Loss: 0.0125, Accuracy: 80.85%\n",
      "Epoch [802/1000], Loss: 0.0322, Accuracy: 79.17%\n",
      "Epoch [803/1000], Loss: 0.0534, Accuracy: 77.55%\n",
      "Epoch [804/1000], Loss: 0.0542, Accuracy: 78.26%\n",
      "Epoch [805/1000], Loss: 0.0904, Accuracy: 80.85%\n",
      "Epoch [806/1000], Loss: 0.0451, Accuracy: 77.55%\n",
      "Epoch [807/1000], Loss: 0.7322, Accuracy: 78.43%\n",
      "Epoch [808/1000], Loss: 0.3853, Accuracy: 78.43%\n",
      "Epoch [809/1000], Loss: 0.0356, Accuracy: 75.56%\n",
      "Epoch [810/1000], Loss: 0.0118, Accuracy: 78.43%\n",
      "Epoch [811/1000], Loss: 0.6159, Accuracy: 78.43%\n",
      "Epoch [812/1000], Loss: 0.1929, Accuracy: 80.77%\n",
      "Epoch [813/1000], Loss: 0.0046, Accuracy: 80.85%\n",
      "Epoch [814/1000], Loss: 0.0168, Accuracy: 75.47%\n",
      "Epoch [815/1000], Loss: 0.0091, Accuracy: 81.63%\n",
      "Epoch [816/1000], Loss: 0.7349, Accuracy: 80.00%\n",
      "Epoch [817/1000], Loss: 0.1000, Accuracy: 80.77%\n",
      "Epoch [818/1000], Loss: 0.1980, Accuracy: 78.26%\n",
      "Epoch [819/1000], Loss: 0.3451, Accuracy: 76.60%\n",
      "Epoch [820/1000], Loss: 0.2587, Accuracy: 71.43%\n",
      "Epoch [821/1000], Loss: 0.9057, Accuracy: 75.47%\n",
      "Epoch [822/1000], Loss: 0.0394, Accuracy: 68.42%\n",
      "Epoch [823/1000], Loss: 0.0457, Accuracy: 78.43%\n",
      "Epoch [824/1000], Loss: 0.2150, Accuracy: 75.00%\n",
      "Epoch [825/1000], Loss: 0.0238, Accuracy: 76.19%\n",
      "Epoch [826/1000], Loss: 0.1018, Accuracy: 78.43%\n",
      "Epoch [827/1000], Loss: 0.2013, Accuracy: 75.00%\n",
      "Epoch [828/1000], Loss: 0.0140, Accuracy: 75.00%\n",
      "Epoch [829/1000], Loss: 0.0046, Accuracy: 76.60%\n",
      "Epoch [830/1000], Loss: 0.0041, Accuracy: 76.00%\n",
      "Epoch [831/1000], Loss: 0.0003, Accuracy: 76.60%\n",
      "Epoch [832/1000], Loss: 0.1519, Accuracy: 76.60%\n",
      "Epoch [833/1000], Loss: 0.8659, Accuracy: 69.39%\n",
      "Epoch [834/1000], Loss: 0.0451, Accuracy: 76.60%\n",
      "Epoch [835/1000], Loss: 0.1119, Accuracy: 80.00%\n",
      "Epoch [836/1000], Loss: 0.0390, Accuracy: 79.25%\n",
      "Epoch [837/1000], Loss: 0.1711, Accuracy: 79.17%\n",
      "Epoch [838/1000], Loss: 0.0126, Accuracy: 75.56%\n",
      "Epoch [839/1000], Loss: 0.0455, Accuracy: 73.08%\n",
      "Epoch [840/1000], Loss: 0.3381, Accuracy: 76.00%\n",
      "Epoch [841/1000], Loss: 0.1824, Accuracy: 74.51%\n",
      "Epoch [842/1000], Loss: 0.1827, Accuracy: 72.73%\n",
      "Epoch [843/1000], Loss: 0.0434, Accuracy: 75.47%\n",
      "Epoch [844/1000], Loss: 0.0170, Accuracy: 76.60%\n",
      "Epoch [845/1000], Loss: 0.4991, Accuracy: 77.55%\n",
      "Epoch [846/1000], Loss: 0.0390, Accuracy: 80.77%\n",
      "Epoch [847/1000], Loss: 0.0751, Accuracy: 78.26%\n",
      "Epoch [848/1000], Loss: 0.1706, Accuracy: 69.39%\n",
      "Epoch [849/1000], Loss: 0.0220, Accuracy: 75.00%\n",
      "Epoch [850/1000], Loss: 0.4319, Accuracy: 76.00%\n",
      "Epoch [851/1000], Loss: 0.0324, Accuracy: 78.43%\n",
      "Epoch [852/1000], Loss: 0.0648, Accuracy: 75.56%\n",
      "Epoch [853/1000], Loss: 0.0065, Accuracy: 76.60%\n",
      "Epoch [854/1000], Loss: 0.1542, Accuracy: 76.60%\n",
      "Epoch [855/1000], Loss: 0.0719, Accuracy: 78.26%\n",
      "Epoch [856/1000], Loss: 0.0386, Accuracy: 74.51%\n",
      "Epoch [857/1000], Loss: 0.2112, Accuracy: 78.43%\n",
      "Epoch [858/1000], Loss: 0.0124, Accuracy: 78.43%\n",
      "Epoch [859/1000], Loss: 0.0116, Accuracy: 81.63%\n",
      "Epoch [860/1000], Loss: 0.0213, Accuracy: 76.00%\n",
      "Epoch [861/1000], Loss: 0.0229, Accuracy: 77.55%\n",
      "Epoch [862/1000], Loss: 0.0122, Accuracy: 78.43%\n",
      "Epoch [863/1000], Loss: 0.0031, Accuracy: 78.43%\n",
      "Epoch [864/1000], Loss: 0.0143, Accuracy: 77.55%\n",
      "Epoch [865/1000], Loss: 0.0311, Accuracy: 78.43%\n",
      "Epoch [866/1000], Loss: 0.1710, Accuracy: 80.85%\n",
      "Epoch [867/1000], Loss: 0.1377, Accuracy: 79.17%\n",
      "Epoch [868/1000], Loss: 0.2784, Accuracy: 78.43%\n",
      "Epoch [869/1000], Loss: 0.0035, Accuracy: 77.55%\n",
      "Epoch [870/1000], Loss: 0.0452, Accuracy: 78.43%\n",
      "Epoch [871/1000], Loss: 0.0246, Accuracy: 79.17%\n",
      "Epoch [872/1000], Loss: 0.0567, Accuracy: 77.55%\n",
      "Epoch [873/1000], Loss: 0.0054, Accuracy: 77.55%\n",
      "Epoch [874/1000], Loss: 0.0722, Accuracy: 77.55%\n",
      "Epoch [875/1000], Loss: 0.0578, Accuracy: 77.55%\n",
      "Epoch [876/1000], Loss: 0.0046, Accuracy: 76.60%\n",
      "Epoch [877/1000], Loss: 0.0646, Accuracy: 76.60%\n",
      "Epoch [878/1000], Loss: 0.1549, Accuracy: 78.26%\n",
      "Epoch [879/1000], Loss: 0.3530, Accuracy: 74.51%\n",
      "Epoch [880/1000], Loss: 0.2281, Accuracy: 76.00%\n",
      "Epoch [881/1000], Loss: 0.0122, Accuracy: 69.77%\n",
      "Epoch [882/1000], Loss: 0.6213, Accuracy: 76.92%\n",
      "Epoch [883/1000], Loss: 0.0049, Accuracy: 78.26%\n",
      "Epoch [884/1000], Loss: 0.6406, Accuracy: 72.00%\n",
      "Epoch [885/1000], Loss: 0.0057, Accuracy: 75.00%\n",
      "Epoch [886/1000], Loss: 0.1532, Accuracy: 77.27%\n",
      "Epoch [887/1000], Loss: 0.3019, Accuracy: 78.43%\n",
      "Epoch [888/1000], Loss: 0.9829, Accuracy: 73.47%\n",
      "Epoch [889/1000], Loss: 0.0337, Accuracy: 59.46%\n",
      "Epoch [890/1000], Loss: 0.0553, Accuracy: 76.92%\n",
      "Epoch [891/1000], Loss: 0.0265, Accuracy: 79.17%\n",
      "Epoch [892/1000], Loss: 0.4267, Accuracy: 73.08%\n",
      "Epoch [893/1000], Loss: 0.0626, Accuracy: 70.59%\n",
      "Epoch [894/1000], Loss: 0.0261, Accuracy: 76.60%\n",
      "Epoch [895/1000], Loss: 0.0397, Accuracy: 76.00%\n",
      "Epoch [896/1000], Loss: 0.0133, Accuracy: 77.55%\n",
      "Epoch [897/1000], Loss: 0.1539, Accuracy: 76.00%\n",
      "Epoch [898/1000], Loss: 0.0583, Accuracy: 76.00%\n",
      "Epoch [899/1000], Loss: 0.0163, Accuracy: 73.91%\n",
      "Epoch [900/1000], Loss: 0.0111, Accuracy: 73.47%\n",
      "Epoch [901/1000], Loss: 0.0218, Accuracy: 76.60%\n",
      "Epoch [902/1000], Loss: 0.8553, Accuracy: 75.56%\n",
      "Epoch [903/1000], Loss: 0.0360, Accuracy: 78.26%\n",
      "Epoch [904/1000], Loss: 0.0115, Accuracy: 70.59%\n",
      "Epoch [905/1000], Loss: 0.0535, Accuracy: 78.26%\n",
      "Epoch [906/1000], Loss: 0.0109, Accuracy: 75.00%\n",
      "Epoch [907/1000], Loss: 0.1625, Accuracy: 76.60%\n",
      "Epoch [908/1000], Loss: 0.1094, Accuracy: 75.56%\n",
      "Epoch [909/1000], Loss: 1.3701, Accuracy: 74.51%\n",
      "Epoch [910/1000], Loss: 0.0560, Accuracy: 77.55%\n",
      "Epoch [911/1000], Loss: 0.0957, Accuracy: 76.60%\n",
      "Epoch [912/1000], Loss: 0.1226, Accuracy: 69.39%\n",
      "Epoch [913/1000], Loss: 0.3176, Accuracy: 75.56%\n",
      "Epoch [914/1000], Loss: 0.0361, Accuracy: 73.17%\n",
      "Epoch [915/1000], Loss: 0.0696, Accuracy: 75.47%\n",
      "Epoch [916/1000], Loss: 0.0137, Accuracy: 78.26%\n",
      "Epoch [917/1000], Loss: 0.8895, Accuracy: 76.60%\n",
      "Epoch [918/1000], Loss: 0.3080, Accuracy: 73.47%\n",
      "Epoch [919/1000], Loss: 0.0130, Accuracy: 75.00%\n",
      "Epoch [920/1000], Loss: 0.1763, Accuracy: 75.00%\n",
      "Epoch [921/1000], Loss: 0.0092, Accuracy: 72.34%\n",
      "Epoch [922/1000], Loss: 0.0340, Accuracy: 76.60%\n",
      "Epoch [923/1000], Loss: 0.0054, Accuracy: 80.00%\n",
      "Epoch [924/1000], Loss: 0.0218, Accuracy: 78.43%\n",
      "Epoch [925/1000], Loss: 0.0357, Accuracy: 77.55%\n",
      "Epoch [926/1000], Loss: 0.0231, Accuracy: 73.47%\n",
      "Epoch [927/1000], Loss: 0.1781, Accuracy: 77.55%\n",
      "Epoch [928/1000], Loss: 0.5148, Accuracy: 77.78%\n",
      "Epoch [929/1000], Loss: 0.0412, Accuracy: 76.60%\n",
      "Epoch [930/1000], Loss: 0.0465, Accuracy: 79.17%\n",
      "Epoch [931/1000], Loss: 0.0727, Accuracy: 76.00%\n",
      "Epoch [932/1000], Loss: 0.1006, Accuracy: 79.17%\n",
      "Epoch [933/1000], Loss: 0.2541, Accuracy: 76.60%\n",
      "Epoch [934/1000], Loss: 0.0209, Accuracy: 76.60%\n",
      "Epoch [935/1000], Loss: 0.2234, Accuracy: 76.60%\n",
      "Epoch [936/1000], Loss: 0.0175, Accuracy: 73.91%\n",
      "Epoch [937/1000], Loss: 0.0807, Accuracy: 70.59%\n",
      "Epoch [938/1000], Loss: 0.1870, Accuracy: 66.67%\n",
      "Epoch [939/1000], Loss: 0.1080, Accuracy: 75.00%\n",
      "Epoch [940/1000], Loss: 0.0759, Accuracy: 79.17%\n",
      "Epoch [941/1000], Loss: 0.2121, Accuracy: 73.08%\n",
      "Epoch [942/1000], Loss: 0.0100, Accuracy: 77.55%\n",
      "Epoch [943/1000], Loss: 0.0091, Accuracy: 77.55%\n",
      "Epoch [944/1000], Loss: 0.1050, Accuracy: 77.55%\n",
      "Epoch [945/1000], Loss: 1.2830, Accuracy: 77.55%\n",
      "Epoch [946/1000], Loss: 0.1623, Accuracy: 80.77%\n",
      "Epoch [947/1000], Loss: 0.0377, Accuracy: 75.56%\n",
      "Epoch [948/1000], Loss: 0.0387, Accuracy: 76.60%\n",
      "Epoch [949/1000], Loss: 0.0794, Accuracy: 77.55%\n",
      "Epoch [950/1000], Loss: 0.1125, Accuracy: 75.00%\n",
      "Epoch [951/1000], Loss: 0.2450, Accuracy: 69.57%\n",
      "Epoch [952/1000], Loss: 0.8934, Accuracy: 73.91%\n",
      "Epoch [953/1000], Loss: 1.0250, Accuracy: 65.00%\n",
      "Epoch [954/1000], Loss: 0.4079, Accuracy: 79.25%\n",
      "Epoch [955/1000], Loss: 0.0106, Accuracy: 63.16%\n",
      "Epoch [956/1000], Loss: 0.0522, Accuracy: 72.00%\n",
      "Epoch [957/1000], Loss: 0.6130, Accuracy: 79.17%\n",
      "Epoch [958/1000], Loss: 0.0367, Accuracy: 80.00%\n",
      "Epoch [959/1000], Loss: 0.2904, Accuracy: 76.60%\n",
      "Epoch [960/1000], Loss: 0.0245, Accuracy: 79.17%\n",
      "Epoch [961/1000], Loss: 0.1434, Accuracy: 75.47%\n",
      "Epoch [962/1000], Loss: 0.0576, Accuracy: 84.00%\n",
      "Epoch [963/1000], Loss: 0.0202, Accuracy: 80.00%\n",
      "Epoch [964/1000], Loss: 0.0557, Accuracy: 78.43%\n",
      "Epoch [965/1000], Loss: 0.0082, Accuracy: 80.00%\n",
      "Epoch [966/1000], Loss: 0.1162, Accuracy: 80.00%\n",
      "Epoch [967/1000], Loss: 0.0502, Accuracy: 80.00%\n",
      "Epoch [968/1000], Loss: 0.3118, Accuracy: 76.60%\n",
      "Epoch [969/1000], Loss: 0.0348, Accuracy: 79.17%\n",
      "Epoch [970/1000], Loss: 0.0061, Accuracy: 80.00%\n",
      "Epoch [971/1000], Loss: 0.0095, Accuracy: 81.63%\n",
      "Epoch [972/1000], Loss: 0.3568, Accuracy: 80.00%\n",
      "Epoch [973/1000], Loss: 0.0519, Accuracy: 72.34%\n",
      "Epoch [974/1000], Loss: 0.3068, Accuracy: 74.51%\n",
      "Epoch [975/1000], Loss: 0.0228, Accuracy: 70.83%\n",
      "Epoch [976/1000], Loss: 0.0071, Accuracy: 76.00%\n",
      "Epoch [977/1000], Loss: 0.0575, Accuracy: 76.92%\n",
      "Epoch [978/1000], Loss: 0.0128, Accuracy: 73.47%\n",
      "Epoch [979/1000], Loss: 0.0711, Accuracy: 73.47%\n",
      "Epoch [980/1000], Loss: 0.2220, Accuracy: 77.55%\n",
      "Epoch [981/1000], Loss: 0.0167, Accuracy: 75.00%\n",
      "Epoch [982/1000], Loss: 0.8223, Accuracy: 72.34%\n",
      "Epoch [983/1000], Loss: 0.0857, Accuracy: 63.41%\n",
      "Epoch [984/1000], Loss: 0.1370, Accuracy: 73.08%\n",
      "Epoch [985/1000], Loss: 0.0094, Accuracy: 80.85%\n",
      "Epoch [986/1000], Loss: 0.6719, Accuracy: 74.07%\n",
      "Epoch [987/1000], Loss: 0.0087, Accuracy: 72.34%\n",
      "Epoch [988/1000], Loss: 0.6171, Accuracy: 74.42%\n",
      "Epoch [989/1000], Loss: 0.4570, Accuracy: 68.00%\n",
      "Epoch [990/1000], Loss: 0.4621, Accuracy: 63.64%\n",
      "Epoch [991/1000], Loss: 0.1047, Accuracy: 61.11%\n",
      "Epoch [992/1000], Loss: 0.0864, Accuracy: 69.23%\n",
      "Epoch [993/1000], Loss: 0.0040, Accuracy: 78.26%\n",
      "Epoch [994/1000], Loss: 0.0561, Accuracy: 76.60%\n",
      "Epoch [995/1000], Loss: 0.2234, Accuracy: 72.00%\n",
      "Epoch [996/1000], Loss: 0.1121, Accuracy: 73.91%\n",
      "Epoch [997/1000], Loss: 0.1138, Accuracy: 76.00%\n",
      "Epoch [998/1000], Loss: 0.0942, Accuracy: 79.17%\n",
      "Epoch [999/1000], Loss: 0.1680, Accuracy: 80.00%\n",
      "Epoch [1000/1000], Loss: 0.7602, Accuracy: 80.00%\n",
      "Best model accuracy: 87.5000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\humming\\AppData\\Local\\Temp\\ipykernel_21196\\3379596615.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load(best_model_path, map_location=torch.device('cpu')))  # map_location 추가 가능\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# 데이터 불러오기\n",
    "train_df = pd.read_excel(\"C:/Users/humming/Downloads/상장폐지 종목 예측 프로젝트/학습데이터/financial_train.xlsx\")\n",
    "test_df = pd.read_excel(\"C:/Users/humming/Downloads/상장폐지 종목 예측 프로젝트/테스트데이터/test.xlsx\")\n",
    "\n",
    "# 종목코드 열 제거 및 피처와 레이블 분리\n",
    "X_train_data = train_df[['총자산', '총자본회전율', '부채비율', '매출액증가율', '자기자본회전율', '당기순이익']]\n",
    "y_train_data = train_df['레이블']\n",
    "\n",
    "X_test_data = test_df[['총자산', '총자본회전율', '부채비율', '매출액증가율', '자기자본회전율', '당기순이익']]\n",
    "y_test_data = test_df['레이블']\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_data)\n",
    "X_test_scaled = scaler.transform(X_test_data)\n",
    "\n",
    "# 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).unsqueeze(1)  # 시퀀스 차원 추가\n",
    "y_train_tensor = torch.tensor(y_train_data.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(1)  # 시퀀스 차원 추가\n",
    "y_test_tensor = torch.tensor(y_test_data.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 데이터셋과 데이터로더 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "# RNN 모델 정의\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.relu(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_size = X_train_tensor.shape[2]  # input_size는 특성(feature)의 수\n",
    "print(input_size,'ddd')\n",
    "hidden_size = 86\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# 모델 초기화, 손실 함수 및 옵티마이저 정의\n",
    "model = RNNModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 최고 성능 기록을 위한 변수 초기화\n",
    "best_f1 = 0.0000\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # 순전파\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # 역전파 및 최적화\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 테스트 데이터로 평가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            y_true.extend(y_batch.tolist())\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "    # 현재 에포크에서의 모델이 최고 성능을 기록했다면 저장\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {f1*100:.2f}%')\n",
    "\n",
    "# 최고 성능 모델 로드\n",
    "best_model = RNNModel(input_size, hidden_size, output_size)\n",
    "best_model.load_state_dict(torch.load(best_model_path, map_location=torch.device('cpu')))  # map_location 추가 가능\n",
    "best_model.eval()\n",
    "\n",
    "\n",
    "print(f'Best model accuracy: {best_f1* 100:.4f}%')\n",
    "\n",
    "# 테스트 데이터에 대해 최종 예측 수행\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(X_test_tensor)\n",
    "    predicted = test_outputs.round()  # 최종 예측 값 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model accuracy: 86.2745%\n"
     ]
    }
   ],
   "source": [
    "print(f'Best model accuracy: {best_f1* 100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9400\n",
      "Precision: 0.8750\n",
      "Recall: 0.8750\n",
      "F1 Score: 0.8750\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "\n",
    "        outputs = best_model(X_batch).squeeze()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        y_pred.extend(predicted.tolist())\n",
    "        y_true.extend(y_batch.tolist())\n",
    "        \n",
    "# 성능 지표 계산\n",
    "accuracy1 = accuracy_score(y_true, y_pred)\n",
    "precision1 = precision_score(y_true, y_pred)\n",
    "recall1 = recall_score(y_true, y_pred)\n",
    "f11 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy1:.4f}')\n",
    "print(f'Precision: {precision1:.4f}')\n",
    "print(f'Recall: {recall1:.4f}')\n",
    "print(f'F1 Score: {f11:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
